{
  "metadata": {
    "generated": "2025-11-05T20:47:24.784650+00:00",
    "source": "Telcofy Documentation",
    "total_documents": 37,
    "description": "LLM-optimized documentation with frontmatter, imports, JSX, and styling removed"
  },
  "documents": [
    {
      "path": "getting-started.md",
      "content": "# ðŸš§ Getting Started - Under Construction ðŸš§\nThis section is currently being updated with the latest installation and setup instructions.\n## Coming Soon\nWe're preparing comprehensive guides for:\n- Installation procedures\n- Initial configuration\n- Quick start tutorials\n- Best practices\nPlease check back soon for updates.\n## Contact Us\nFor immediate assistance with getting started:\n- ðŸ“§ Email: tom@telcofy.ai\n- ðŸ™ GitHub: [github.com/telcofy](https://github.com/telcofy)"
    },
    {
      "path": "index.mdx",
      "content": "Telcofy Documentation\n  Transform Telco Data into Value - EU Compliance & Analytics Platform\n---\n## Products\n- [Telcofy Product Suite](./products/index.md)\n## Methodology\n- Explore the [methodology section](./methodology/index.md) for analytical frameworks and best practices.\n## EU Compliance\nTelcofy aligns with Eurostat's MultiMNO standards:\n- [EU Compliance overview](./eu-compliance/index.md)\n- [Methodological Framework (Vol. I)](./eu-compliance/methodological-framework.md)\n- [Use Cases (Vol. II)](./eu-compliance/use-cases.md)\n- [Methods & Data Objects (Vol. III)](./eu-compliance/methods.md)\n- [Codebase & Orchestration](./eu-compliance/codebase-overview.md)\n- [Quality Framework](./eu-compliance/quality.md)\n- [Staypoint Detection](./eu-compliance/eurostat-pipeline-staypoint-detection.md)\n## Data Delivery\n- [Data Delivery Overview](./data-access/overview.md)\n- [Google Bigquery Sharing](./data-access/analytical-hub.md)\n## API Reference\n- [Quickstart](./api/quickstart.md)\n- [Authentication](./api/authentication.md)\n- [Endpoints](./api/endpoints.md)\n---"
    },
    {
      "path": "products/index.md",
      "content": "# Telcofy Product Suite\nTelcofy packages MultiMNO-derived analytics into commercial offerings designed for business and public-sector decision makers. Every dataset can split local SIMs from foreign roamers (with country-of-origin metadata). Local populations may be extrapolated using the originating operatorâ€™s market-share factors, whereas foreign roamers remain un-extrapolated because cross-operator coverage cannot be observed.\n## Standardised Activity Stays\n- **What it measures** â€” Sustained device presence based on staypoint detection (e.g. devices dwelling longer than 10â€¯minutes).\n- **Use cases** â€” Urban planning, retail footfall benchmarking, commercial real-estate evaluation, event and venue analysis.\n- **Delivery** â€” Time-bucketed activity tables with duration bands, dwell counts, and optional segmentation by local vs. foreign SIMs.\n## ODM: Originâ€“Destination Matrix\n- **What it measures** â€” Movement flows between stay locations, capturing departures, arrivals, and trip volumes within configurable time windows.\n- **Use cases** â€” Regional and long-distance transport planning, corridor optimisation, tourism flow analysis, infrastructure investment targeting.\n- **Delivery** â€” Aggregated OD matrices with device counts, journey-time stats, and origin/destination attributes (including country for foreign SIMs).\n## Telcofy Realtime\n- **What it measures** â€” Low-latency snapshots of unique devices in a geography.\n- **Use cases** â€” Operational monitoring, traffic and crowd management, emergency response, live event operations.\n- **Delivery** â€” Streaming or frequent snapshots, surfaced via dashboards or APIs, with optional local vs. foreign breakdowns (non-extrapolated for roamers)."
    },
    {
      "path": "methodology/analytics-insights.md",
      "content": "# Analytics & Insights Generation\nHow Telcofy transforms processed telecommunications data into meaningful insights for business intelligence and regulatory reporting.\n## Analytics Pipeline Overview\nOur analytics methodology converts anonymized, compliant data into actionable insights through sophisticated algorithms and statistical methods.\n## Core Analytics Functions\n### Population Density Analysis\n```pseudocode\nFUNCTION calculate_population_density(records, grid_size)\n    density_map = initialize_grid(grid_size)\n    FOR each record IN records\n        grid_cell = map_to_grid(record.location, grid_size)\n        density_map[grid_cell] += record.weighted_count\n    END FOR\n    # Apply smoothing to prevent re-identification\n    smoothed_density = apply_gaussian_smoothing(density_map)\n    RETURN smoothed_density\nEND FUNCTION\n```\n### Mobility Pattern Analysis\n```pseudocode\nFUNCTION analyze_mobility_patterns(records)\n    patterns = {}\n    # Group by time periods\n    hourly_patterns = group_by_hour(records)\n    daily_patterns = group_by_day(records)\n    # Calculate origin-destination matrices\n    od_matrix = calculate_od_matrix(records)\n    # Identify commuting patterns\n    commuting_flows = identify_commuting_flows(od_matrix)\n    patterns.hourly = hourly_patterns\n    patterns.daily = daily_patterns\n    patterns.commuting = commuting_flows\n    RETURN patterns\nEND FUNCTION\n```\n## Statistical Analysis Methods\n### Time Series Analysis\n```pseudocode\nFUNCTION analyze_temporal_trends(time_series_data)\n    trends = {}\n    # Decompose time series\n    trends.seasonal = extract_seasonal_patterns(time_series_data)\n    trends.trend = extract_long_term_trend(time_series_data)\n    trends.residual = calculate_residuals(time_series_data)\n    # Forecast future values\n    trends.forecast = generate_forecasts(time_series_data)\n    RETURN trends\nEND FUNCTION\n```\n### Spatial Analysis\n```pseudocode\nFUNCTION perform_spatial_analysis(spatial_data)\n    spatial_insights = {}\n    # Hotspot analysis\n    spatial_insights.hotspots = identify_activity_hotspots(spatial_data)\n    # Connectivity analysis\n    spatial_insights.connectivity = analyze_spatial_connectivity(spatial_data)\n    # Accessibility metrics\n    spatial_insights.accessibility = calculate_accessibility_metrics(spatial_data)\n    RETURN spatial_insights\nEND FUNCTION\n```\n## Business Intelligence Generation\n### Tourism Analytics\n```pseudocode\nFUNCTION analyze_tourism_patterns(mobility_data, external_data)\n    tourism_insights = {}\n    # Identify tourist vs resident patterns\n    tourism_insights.visitor_classification = classify_visitors(mobility_data)\n    # Analyze stay duration\n    tourism_insights.stay_duration = calculate_stay_patterns(mobility_data)\n    # Identify popular destinations\n    tourism_insights.popular_destinations = rank_destinations(mobility_data)\n    # Correlate with events and seasons\n    tourism_insights.seasonal_patterns = correlate_with_calendar(mobility_data, external_data)\n    RETURN tourism_insights\nEND FUNCTION\n```\n### Economic Impact Assessment\n```pseudocode\nFUNCTION assess_economic_impact(mobility_data, economic_data)\n    economic_insights = {}\n    # Business district activity\n    economic_insights.business_activity = analyze_business_district_activity(mobility_data)\n    # Retail footfall analysis\n    economic_insights.retail_footfall = analyze_retail_patterns(mobility_data)\n    # Economic correlation\n    economic_insights.economic_correlation = correlate_mobility_economics(mobility_data, economic_data)\n    RETURN economic_insights\nEND FUNCTION\n```\n## Machine Learning Integration\n### Predictive Analytics\n```pseudocode\nFUNCTION generate_predictions(historical_data, prediction_horizon)\n    prediction_models = {}\n    # Traffic prediction\n    traffic_model = train_traffic_prediction_model(historical_data.traffic)\n    prediction_models.traffic = predict_future_traffic(traffic_model, prediction_horizon)\n    # Demand forecasting\n    demand_model = train_demand_forecasting_model(historical_data.demand)\n    prediction_models.demand = forecast_demand(demand_model, prediction_horizon)\n    # Anomaly detection\n    anomaly_model = train_anomaly_detection_model(historical_data)\n    prediction_models.anomalies = detect_future_anomalies(anomaly_model, prediction_horizon)\n    RETURN prediction_models\nEND FUNCTION\n```\n### Clustering Analysis\n```pseudocode\nFUNCTION perform_clustering_analysis(feature_data)\n    clustering_results = {}\n    # User behavior clustering\n    clustering_results.user_segments = cluster_user_behaviors(feature_data.user_features)\n    # Geographic clustering\n    clustering_results.geographic_segments = cluster_geographic_areas(feature_data.geographic_features)\n    # Temporal clustering\n    clustering_results.temporal_patterns = cluster_temporal_patterns(feature_data.temporal_features)\n    RETURN clustering_results\nEND FUNCTION\n```\n## Real-Time Analytics\n### Stream Processing\n```pseudocode\nFUNCTION process_real_time_stream(data_stream)\n    real_time_insights = {}\n    # Initialize sliding windows\n    hourly_window = initialize_sliding_window(duration=1_hour)\n    daily_window = initialize_sliding_window(duration=24_hours)\n    FOR each new_data_point IN data_stream\n        # Update sliding windows\n        hourly_window.add(new_data_point)\n        daily_window.add(new_data_point)\n        # Calculate real-time metrics\n        real_time_insights.current_density = calculate_current_density(hourly_window)\n        real_time_insights.trend_direction = calculate_trend_direction(daily_window)\n        real_time_insights.anomaly_score = calculate_anomaly_score(new_data_point, hourly_window)\n        # Trigger alerts if necessary\n        IF real_time_insights.anomaly_score > ANOMALY_THRESHOLD THEN\n            trigger_anomaly_alert(real_time_insights)\n        END IF\n    END FOR\n    RETURN real_time_insights\nEND FUNCTION\n```\n## Quality Metrics for Analytics\n### Accuracy Assessment\n```pseudocode\nFUNCTION assess_analytics_accuracy(predictions, ground_truth)\n    accuracy_metrics = {}\n    # Statistical accuracy measures\n    accuracy_metrics.mae = calculate_mean_absolute_error(predictions, ground_truth)\n    accuracy_metrics.rmse = calculate_root_mean_square_error(predictions, ground_truth)\n    accuracy_metrics.mape = calculate_mean_absolute_percentage_error(predictions, ground_truth)\n    # Business accuracy measures\n    accuracy_metrics.directional_accuracy = calculate_directional_accuracy(predictions, ground_truth)\n    accuracy_metrics.confidence_intervals = calculate_confidence_intervals(predictions)\n    RETURN accuracy_metrics\nEND FUNCTION\n```\n### Insights Validation\n```pseudocode\nFUNCTION validate_insights(generated_insights, validation_data)\n    validation_results = {}\n    # Cross-validation\n    validation_results.cross_validation = perform_cross_validation(generated_insights, validation_data)\n    # External validation\n    validation_results.external_validation = validate_against_external_sources(generated_insights)\n    # Business logic validation\n    validation_results.business_validation = validate_business_logic(generated_insights)\n    RETURN validation_results\nEND FUNCTION\n```\n## Integration with Eurostat Requirements\n### Statistical Output Formatting\n```pseudocode\nFUNCTION format_for_statistical_reporting(analytics_output)\n    statistical_report = {}\n    # Apply Eurostat classifications\n    statistical_report.nuts_regions = apply_nuts_classification(analytics_output.geographic_data)\n    statistical_report.time_periods = standardize_time_periods(analytics_output.temporal_data)\n    # Aggregate to required statistical levels\n    statistical_report.aggregated_metrics = aggregate_to_statistical_units(analytics_output)\n    # Apply disclosure control\n    statistical_report.protected_data = apply_statistical_disclosure_control(statistical_report.aggregated_metrics)\n    RETURN statistical_report\nEND FUNCTION\n```\n## Performance Optimization for Analytics\n### Computational Efficiency\n```pseudocode\nFUNCTION optimize_analytics_performance(large_dataset)\n    # Implement distributed computing\n    partitioned_data = partition_dataset(large_dataset)\n    # Parallel processing\n    partial_results = []\n    FOR each partition IN partitioned_data PARALLEL\n        partial_result = compute_analytics(partition)\n        partial_results.append(partial_result)\n    END FOR\n    # Aggregate results\n    final_analytics = aggregate_partial_results(partial_results)\n    RETURN final_analytics\nEND FUNCTION\n```\nThis analytics methodology ensures that Telcofy generates high-quality, actionable insights while maintaining compliance with privacy regulations and statistical standards."
    },
    {
      "path": "methodology/data-pipeline.md",
      "content": "# Data Processing Pipeline\nThe backbone of Telcofy's data transformation process, handling billions of telecommunications events while maintaining quality and compliance.\n## Pipeline Overview\nOur data processing pipeline consists of four main stages:\n1. **Data Ingestion** - Continuous intake of raw telco data\n2. **Validation & Cleansing** - Quality control and standardization\n3. **Anonymization** - Privacy protection measures\n4. **Compliance Processing** - EU regulation adherence\n## Stage 1: Data Ingestion\n```pseudocode\nFUNCTION ingest_telco_data(raw_data_stream)\n    FOR each data_batch IN raw_data_stream\n        validate_schema(data_batch)\n        apply_initial_filters(data_batch)\n        queue_for_processing(data_batch)\n    END FOR\nEND FUNCTION\n```\n### Data Sources\nThe system continuously ingests data from various telco sources:\n- **Call Detail Records (CDR)**: Voice and SMS transaction data\n- **Location Data**: Cell tower and GPS positioning information\n- **Network Events**: Connection, handover, and quality metrics\n- **Billing Records**: Usage and payment information\n### Ingestion Characteristics\n- **Real-time processing**: Sub-second latency for critical events\n- **Batch processing**: Efficient handling of large historical datasets\n- **Schema validation**: Ensuring data structure compliance\n- **Rate limiting**: Preventing system overload\n## Stage 2: Data Validation & Cleansing\n```pseudocode\nFUNCTION validate_and_cleanse(data_batch)\n    validated_records = []\n    FOR each record IN data_batch\n        IF validate_required_fields(record) THEN\n            cleansed_record = apply_data_cleaning_rules(record)\n            standardized_record = standardize_formats(cleansed_record)\n            validated_records.append(standardized_record)\n        ELSE\n            log_validation_error(record)\n        END IF\n    END FOR\n    RETURN validated_records\nEND FUNCTION\n```\n### Validation Steps\n- **Schema validation**: Ensuring data structure compliance\n- **Data type validation**: Verifying field formats (dates, numbers, strings)\n- **Business rule validation**: Checking logical constraints\n- **Duplicate detection**: Identifying and handling duplicate records\n- **Completeness checks**: Ensuring required fields are present\n### Data Cleansing Rules\n- **Standardization**: Converting to consistent formats\n- **Normalization**: Scaling numerical values appropriately\n- **Outlier detection**: Identifying and handling anomalous values\n- **Missing value handling**: Imputation or exclusion strategies\n## Stage 3: Anonymization Integration\n```pseudocode\nFUNCTION prepare_for_anonymization(validated_records)\n    prepared_records = []\n    FOR each record IN validated_records\n        # Add anonymization metadata\n        record.privacy_level = determine_privacy_level(record)\n        record.anonymization_strategy = select_strategy(record)\n        prepared_records.append(record)\n    END FOR\n    RETURN prepared_records\nEND FUNCTION\n```\nThe pipeline seamlessly integrates with our [Privacy & Anonymization](./privacy-anonymization) module to ensure data protection.\n## Stage 4: Compliance Processing\n```pseudocode\nFUNCTION ensure_pipeline_compliance(processed_records)\n    compliant_records = []\n    FOR each record IN processed_records\n        # Apply GDPR requirements\n        gdpr_compliant_record = apply_gdpr_rules(record)\n        # Format for Eurostat requirements\n        eurostat_formatted = format_for_eurostat(gdpr_compliant_record)\n        # Validate against EU regulations\n        IF validate_eu_compliance(eurostat_formatted) THEN\n            compliant_records.append(eurostat_formatted)\n        END IF\n    END FOR\n    RETURN compliant_records\nEND FUNCTION\n```\n## Performance Characteristics\n### Throughput Metrics\n- **Peak processing rate**: 1M records/second\n- **Average latency**: &lt;100ms per record\n- **Batch processing**: 10TB+ datasets in &lt;1 hour\n- **Availability**: 99.9% uptime SLA\n### Scalability Features\n- **Horizontal scaling**: Auto-scaling based on load\n- **Partitioning**: Intelligent data distribution\n- **Caching**: Multi-level caching for frequently accessed data\n- **Load balancing**: Distributed processing across nodes\n## Error Handling\n```pseudocode\nFUNCTION handle_pipeline_errors(error_context)\n    error_type = classify_error(error_context)\n    SWITCH error_type\n        CASE \"validation_error\":\n            log_validation_issue(error_context)\n            quarantine_record(error_context.record)\n        CASE \"processing_error\":\n            retry_with_backoff(error_context.operation)\n        CASE \"system_error\":\n            alert_operations_team(error_context)\n            failover_to_backup_system()\n        DEFAULT:\n            log_unknown_error(error_context)\n    END SWITCH\nEND FUNCTION\n```\n## Monitoring & Observability\nThe pipeline includes comprehensive monitoring:\n- **Real-time metrics**: Processing rates, error rates, latency\n- **Quality indicators**: Data completeness, accuracy scores\n- **Compliance tracking**: Regulation adherence metrics\n- **Performance dashboards**: System health visualization\n## Integration Points\nThe data pipeline integrates with:\n- [Quality Assurance](./quality-assurance) - Continuous quality monitoring\n- [Analytics & Insights](./analytics-insights) - Downstream processing\n- Performance Optimization - Efficiency improvements *(Coming soon)*\n- API Integration - External data access *(Coming soon)*"
    },
    {
      "path": "methodology/eu-compliance.md",
      "content": "# EU Compliance Methodology\nEnsuring adherence to GDPR, Eurostat requirements, and other EU regulations throughout the data processing lifecycle.\n## Compliance Framework\nTelcofy's compliance methodology is built around three pillars:\n1. **Regulatory Adherence**: Meeting all applicable EU regulations\n2. **Audit Trail**: Maintaining comprehensive documentation\n3. **Continuous Monitoring**: Ongoing compliance validation\n## GDPR Compliance\n```pseudocode\nFUNCTION ensure_gdpr_compliance(data_processing_activity)\n    compliance_checklist = {}\n    # Lawful basis assessment\n    compliance_checklist.lawful_basis = verify_lawful_basis(data_processing_activity)\n    # Data minimization\n    compliance_checklist.data_minimization = verify_data_minimization(data_processing_activity)\n    # Purpose limitation\n    compliance_checklist.purpose_limitation = verify_purpose_limitation(data_processing_activity)\n    # Storage limitation\n    compliance_checklist.storage_limitation = verify_retention_periods(data_processing_activity)\n    # Accuracy principle\n    compliance_checklist.accuracy = verify_data_accuracy(data_processing_activity)\n    # Integrity and confidentiality\n    compliance_checklist.security = verify_security_measures(data_processing_activity)\n    # Accountability\n    compliance_checklist.accountability = verify_documentation(data_processing_activity)\n    RETURN validate_all_requirements(compliance_checklist)\nEND FUNCTION\n```\n### Data Subject Rights Implementation\n```pseudocode\nFUNCTION handle_data_subject_request(request_type, subject_identifier)\n    SWITCH request_type\n        CASE \"access\":\n            return provide_data_access(subject_identifier)\n        CASE \"rectification\":\n            return process_data_correction(subject_identifier, request.corrections)\n        CASE \"erasure\":\n            return process_right_to_be_forgotten(subject_identifier)\n        CASE \"portability\":\n            return provide_data_export(subject_identifier, request.format)\n        CASE \"restriction\":\n            return restrict_data_processing(subject_identifier)\n        CASE \"objection\":\n            return process_objection_to_processing(subject_identifier)\n        DEFAULT:\n            return log_invalid_request(request)\n    END SWITCH\nEND FUNCTION\n```\n## Eurostat Compliance\n```pseudocode\nFUNCTION format_for_eurostat(compliance_data)\n    eurostat_formatted = {}\n    # Apply Eurostat classification standards\n    eurostat_formatted.geography = apply_nuts_classification(compliance_data.location)\n    eurostat_formatted.time_period = standardize_time_format(compliance_data.timestamp)\n    eurostat_formatted.statistical_unit = define_statistical_unit(compliance_data)\n    # Apply statistical disclosure control\n    eurostat_formatted.data = apply_disclosure_control(compliance_data.aggregated_values)\n    # Add metadata required by Eurostat\n    eurostat_formatted.metadata = generate_eurostat_metadata(compliance_data)\n    # Validate against Eurostat schemas\n    IF validate_eurostat_schema(eurostat_formatted) THEN\n        RETURN eurostat_formatted\n    ELSE\n        RETURN handle_validation_errors(eurostat_formatted)\n    END IF\nEND FUNCTION\n```\n### Statistical Disclosure Control\n```pseudocode\nFUNCTION apply_disclosure_control(statistical_data)\n    controlled_data = statistical_data\n    # Primary disclosure control\n    controlled_data = apply_primary_suppression(controlled_data)\n    # Secondary disclosure control\n    controlled_data = apply_complementary_suppression(controlled_data)\n    # Cell perturbation for additional protection\n    controlled_data = apply_cell_perturbation(controlled_data)\n    # Validate disclosure risk\n    IF assess_disclosure_risk(controlled_data) > ACCEPTABLE_RISK_THRESHOLD THEN\n        controlled_data = apply_additional_protection(controlled_data)\n    END IF\n    RETURN controlled_data\nEND FUNCTION\n```\n## ePrivacy Directive Compliance\n```pseudocode\nFUNCTION ensure_eprivacy_compliance(communication_data)\n    compliance_status = {}\n    # Confidentiality of communications\n    compliance_status.confidentiality = verify_communication_confidentiality(communication_data)\n    # Traffic data processing\n    compliance_status.traffic_data = verify_traffic_data_processing(communication_data)\n    # Location data processing\n    compliance_status.location_data = verify_location_data_consent(communication_data)\n    # Terminal equipment access\n    compliance_status.terminal_access = verify_terminal_equipment_consent(communication_data)\n    RETURN validate_eprivacy_requirements(compliance_status)\nEND FUNCTION\n```\n## Consent Management\n```pseudocode\nFUNCTION manage_consent(user_identifier, processing_purposes)\n    consent_record = {}\n    # Verify consent requirements\n    required_consents = determine_consent_requirements(processing_purposes)\n    # Check existing consents\n    existing_consents = retrieve_user_consents(user_identifier)\n    FOR each purpose IN processing_purposes\n        IF purpose_requires_consent(purpose) THEN\n            IF valid_consent_exists(existing_consents, purpose) THEN\n                consent_record[purpose] = \"valid\"\n            ELSE\n                consent_record[purpose] = \"required\"\n                request_user_consent(user_identifier, purpose)\n            END IF\n        ELSE\n            consent_record[purpose] = \"not_required\"\n        END IF\n    END FOR\n    RETURN consent_record\nEND FUNCTION\n```\n## Data Protection Impact Assessment (DPIA)\n```pseudocode\nFUNCTION conduct_dpia(processing_activity)\n    dpia_assessment = {}\n    # Step 1: Necessity and proportionality assessment\n    dpia_assessment.necessity = assess_necessity(processing_activity)\n    dpia_assessment.proportionality = assess_proportionality(processing_activity)\n    # Step 2: Risk identification\n    dpia_assessment.privacy_risks = identify_privacy_risks(processing_activity)\n    dpia_assessment.rights_impact = assess_rights_impact(processing_activity)\n    # Step 3: Risk mitigation measures\n    dpia_assessment.mitigation_measures = design_mitigation_measures(dpia_assessment.privacy_risks)\n    # Step 4: Residual risk assessment\n    dpia_assessment.residual_risks = assess_residual_risks(dpia_assessment)\n    # Step 5: Decision making\n    IF dpia_assessment.residual_risks > ACCEPTABLE_RISK_LEVEL THEN\n        dpia_assessment.recommendation = \"consult_supervisory_authority\"\n    ELSE\n        dpia_assessment.recommendation = \"proceed_with_safeguards\"\n    END IF\n    RETURN dpia_assessment\nEND FUNCTION\n```\n## Cross-Border Data Transfer Compliance\n```pseudocode\nFUNCTION ensure_transfer_compliance(data, destination_country)\n    transfer_mechanism = determine_transfer_mechanism(destination_country)\n    SWITCH transfer_mechanism\n        CASE \"adequacy_decision\":\n            return validate_adequacy_decision(destination_country)\n        CASE \"standard_contractual_clauses\":\n            return implement_sccs(data, destination_country)\n        CASE \"binding_corporate_rules\":\n            return validate_bcrs(data, destination_country)\n        CASE \"certification_mechanism\":\n            return validate_certification(data, destination_country)\n        CASE \"derogation\":\n            return apply_derogation(data, destination_country)\n        DEFAULT:\n            return block_transfer(data, destination_country)\n    END SWITCH\nEND FUNCTION\n```\n## Compliance Monitoring and Reporting\n```pseudocode\nFUNCTION monitor_compliance_status()\n    compliance_metrics = {}\n    # GDPR compliance metrics\n    compliance_metrics.gdpr = {\n        data_subject_requests_response_time: calculate_average_response_time(),\n        data_minimization_compliance: assess_data_minimization_compliance(),\n        security_incidents: count_security_incidents(),\n        breach_notification_compliance: assess_breach_notification_compliance()\n    # Eurostat compliance metrics\n    compliance_metrics.eurostat = {\n        reporting_deadline_compliance: assess_reporting_deadlines(),\n        data_quality_scores: calculate_data_quality_scores(),\n        statistical_confidentiality: assess_confidentiality_protection()\n    # ePrivacy compliance metrics\n    compliance_metrics.eprivacy = {\n        consent_rates: calculate_consent_rates(),\n        communication_confidentiality: assess_confidentiality_measures(),\n        terminal_equipment_compliance: assess_terminal_access_compliance()\n    RETURN compliance_metrics\nEND FUNCTION\n```\n## Automated Compliance Validation\n```pseudocode\nFUNCTION validate_compliance_automatically(processed_data)\n    validation_results = {}\n    # Automated GDPR validation\n    validation_results.gdpr = run_gdpr_validation_rules(processed_data)\n    # Automated Eurostat validation\n    validation_results.eurostat = run_eurostat_validation_rules(processed_data)\n    # Automated data quality validation\n    validation_results.data_quality = run_quality_validation_rules(processed_data)\n    # Generate compliance report\n    compliance_report = generate_compliance_report(validation_results)\n    # Alert on non-compliance\n    IF detect_non_compliance(validation_results) THEN\n        trigger_compliance_alert(validation_results)\n    END IF\n    RETURN compliance_report\nEND FUNCTION\n```\n## Audit Trail Management\n```pseudocode\nFUNCTION maintain_audit_trail(activity)\n    audit_entry = {\n        timestamp: current_timestamp(),\n        activity_type: activity.type,\n        user_identifier: activity.user,\n        data_affected: activity.data_scope,\n        legal_basis: activity.legal_basis,\n        processing_purpose: activity.purpose,\n        retention_period: activity.retention,\n        security_measures: activity.security_controls\n    # Encrypt audit entry\n    encrypted_entry = encrypt_audit_entry(audit_entry)\n    # Store in tamper-evident audit log\n    store_audit_entry(encrypted_entry)\n    # Update compliance dashboard\n    update_compliance_dashboard(audit_entry)\n    RETURN audit_entry.id\nEND FUNCTION\n```\n## Integration with Privacy Framework\nOur EU compliance methodology works in close integration with:\n- [Privacy & Anonymization](./privacy-anonymization) - Technical privacy measures\n- [Quality Assurance](./quality-assurance) - Compliance validation\n- [Data Processing Pipeline](./data-pipeline) - Compliance by design\n- API Integration - Compliant data access *(Coming soon)*\nThis comprehensive approach ensures that Telcofy not only meets current EU regulatory requirements but is also prepared for evolving privacy and data protection regulations."
    },
    {
      "path": "methodology/index.md",
      "content": "# Telcofy Methodology Overview\nTelcofyâ€™s analytics follow the Eurostat **MultiMNO** methodology. The summaries below use business-friendly names, with the original EU terminology in brackets so analysts can cross-check the detailed specifications in the [EU Compliance library](../eu-compliance/index.md) and [Use Case catalogue](../eu-compliance/use-cases.md).\n## Activity Stays (Staypoint Detection)\n- **What we do** â€” detect locations where devices linger for meaningful periods (for example, longer than 10â€¯minutes) by analysing cleaned event streams.\n- **Why it matters** â€” reveals how busy a site is and how long visitors remain, powering retail footfall studies, venue performance reviews, and destination benchmarking.\n- **Tech notes** â€” Implementation details, including event ingestion, windowing, and cache handling, are described in the [Staypoint Detection guide](../eu-compliance/eurostat-pipeline-staypoint-detection.md) and the processing modules in [Methods & Data Objects](../eu-compliance/methods.md).\n## Population Snapshots (Present Population Estimation)\n- **What we do** â€” estimate how many unique devices are present in every grid tile at defined timestamps using Bayesian weighting against cell-coverage probabilities.\n- **Why it matters** â€” provides crowd counts for city operations, event management, safety monitoring, and infrastructure planning.\n- **Tech notes** â€” See Moduleâ€¯13 in [Methods & Data Objects](../eu-compliance/methods.md) for tolerance windows, iteration thresholds, and quality outputs referenced by NSIs.\n## Movement Segments (Continuous Time Segmentation)\n- **What we do** â€” label each stretch of time as STAY, MOVE, ABROAD, or UNKNOWN, generating a high-resolution activity timeline.\n- **Why it matters** â€” unlocks commuting routines, trip sequences, peak travel windows, and visit sequencing.\n- **Tech notes** â€” The [Staypoint Detection guide](../eu-compliance/eurostat-pipeline-staypoint-detection.md) details key inputs (semantic-cleaned events, intersection groups), **segmentation parameters** (minimum stay duration, maximum gaps, domain filters, MCC rules), and **state assignment** logic that produces labelled segments with continuity markers.\n## Usual Places (M-Usual Environment Indicators)\n- **What we do** â€” roll up movement segments over weeks and months to determine peopleâ€™s habitual locations (home, work/school, and other frequented spots).\n- **Why it matters** â€” exposes catchment areas, workplace concentrations, secondary-home patterns, and tourist routines for planning and marketing teams.\n- **Tech notes** â€” Mid-term aggregation settings, confidence measures, and recommended outputs are covered in Moduleâ€¯14 of [Methods & Data Objects](../eu-compliance/methods.md) and the relevant use cases in [Vol.â€¯II](../eu-compliance/use-cases.md).\n## Home Base Identification (M-Home Location Indicators)\n- **What we do** â€” pinpoint each deviceâ€™s likely home using long-term permanence scores, with confidence metrics and change alerts.\n- **Why it matters** â€” offers accurate residential baselines for real-estate analysis, public-service targeting, audience segmentation, and churn detection.\n- **Tech notes** â€” Moduleâ€¯15 in [Methods & Data Objects](../eu-compliance/methods.md) outlines the scoring thresholds, quality metrics, and metadata fields shared with NSIs.\n## Relocation Tracking (Internal Migration)\n- **What we do** â€” compare historical home bases to flag moves between districts or cities, distinguishing temporary from sustained relocations.\n- **Why it matters** â€” informs housing strategy, infrastructure investment, workforce planning, and regional development initiatives.\n- **Tech notes** â€” Migration computation, confidence flags, and reporting templates are defined in Moduleâ€¯16/17 of [Methods & Data Objects](../eu-compliance/methods.md) and the migration section of [Use Cases](../eu-compliance/use-cases.md).\n## Putting it all together\n1. **Activity Stays** and **Movement Segments** produce the raw behavioural timeline.\n2. **Population Snapshots** and **Usual Places** aggregate those timelines into daily and monthly intelligence.\n3. **Home Base Identification** and **Relocation Tracking** deliver long-term insights, enabling products such as OD matrices and real-time dashboards showcased in the [Telcofy Product Suite](../products/index.md).\nAll modules honour privacy constraints, separate local versus foreign SIMs, and support market-share extrapolation where feasible, ensuring results are trustworthy for commercial and technical teams alike."
    },
    {
      "path": "methodology/privacy-anonymization.md",
      "content": "# Privacy & Anonymization\nAdvanced techniques to protect individual privacy while maintaining data utility for telecommunications analytics.\n## Privacy-First Approach\nTelcofy implements privacy protection at every stage of data processing, ensuring that individual privacy is preserved while enabling valuable insights for telecommunications analytics.\n## Core Anonymization Techniques\n### 1. K-Anonymity\n```pseudocode\nFUNCTION apply_k_anonymity(records, k_value)\n    anonymized_records = []\n    grouped_records = group_by_quasi_identifiers(records)\n    FOR each group IN grouped_records\n        IF group.size >= k_value THEN\n            generalized_group = generalize_attributes(group)\n            anonymized_records.extend(generalized_group)\n        ELSE\n            # Group too small, apply additional generalization\n            merged_group = merge_with_similar_group(group, grouped_records)\n            generalized_group = generalize_attributes(merged_group)\n            anonymized_records.extend(generalized_group)\n        END IF\n    END FOR\n    RETURN anonymized_records\nEND FUNCTION\n```\n**K-anonymity ensures**: Each record is indistinguishable from at least k-1 other records based on quasi-identifying attributes.\n### 2. Differential Privacy\n```pseudocode\nFUNCTION apply_differential_privacy(data, epsilon, query_function)\n    # Calculate sensitivity of the query\n    sensitivity = calculate_sensitivity(query_function)\n    # Add calibrated Laplace noise\n    noise_scale = sensitivity / epsilon\n    noise = generate_laplace_noise(noise_scale)\n    # Apply query and add noise\n    true_result = query_function(data)\n    private_result = true_result + noise\n    RETURN private_result\nEND FUNCTION\n```\n**Differential privacy provides**: Mathematical guarantees that the presence or absence of any individual record cannot be determined from the output.\n### 3. Spatial Generalization\n```pseudocode\nFUNCTION spatially_generalize(location_data, grid_size)\n    generalized_locations = []\n    FOR each location IN location_data\n        # Map to grid cell\n        grid_cell = map_to_grid(location, grid_size)\n        # Use grid cell center as generalized location\n        generalized_location = get_grid_center(grid_cell)\n        generalized_locations.append(generalized_location)\n    END FOR\n    RETURN generalized_locations\nEND FUNCTION\n```\n**Spatial generalization**: Reduces location precision while maintaining geographic utility.\n## Comprehensive Anonymization Pipeline\n```pseudocode\nFUNCTION anonymize_data(validated_records)\n    anonymized_records = []\n    FOR each record IN validated_records\n        # Step 1: Remove direct identifiers\n        sanitized_record = remove_direct_identifiers(record)\n        # Step 2: Apply spatial generalization\n        spatially_generalized = generalize_location(sanitized_record)\n        # Step 3: Apply temporal generalization\n        temporally_generalized = generalize_time(spatially_generalized)\n        # Step 4: Apply k-anonymity\n        k_anonymous_record = apply_k_anonymity_single(temporally_generalized)\n        # Step 5: Add differential privacy noise\n        noisy_record = add_calibrated_noise(k_anonymous_record)\n        anonymized_records.append(noisy_record)\n    END FOR\n    RETURN anonymized_records\nEND FUNCTION\n```\n## Direct Identifier Removal\n```pseudocode\nFUNCTION remove_direct_identifiers(record)\n    # Remove or hash direct identifiers\n    record.phone_number = hash_with_salt(record.phone_number)\n    record.imsi = hash_with_salt(record.imsi)\n    record.imei = hash_with_salt(record.imei)\n    # Remove names and addresses\n    record.customer_name = null\n    record.billing_address = null\n    # Keep only necessary operational identifiers\n    record.session_id = generate_anonymous_session_id()\n    RETURN record\nEND FUNCTION\n```\n## Temporal Generalization\n```pseudocode\nFUNCTION generalize_time(record, time_granularity)\n    original_timestamp = record.timestamp\n    SWITCH time_granularity\n        CASE \"hour\":\n            generalized_time = round_to_hour(original_timestamp)\n        CASE \"day\":\n            generalized_time = round_to_day(original_timestamp)\n        CASE \"week\":\n            generalized_time = round_to_week(original_timestamp)\n        DEFAULT:\n            generalized_time = round_to_hour(original_timestamp)\n    END SWITCH\n    record.timestamp = generalized_time\n    RETURN record\nEND FUNCTION\n```\n## Privacy Level Assessment\n```pseudocode\nFUNCTION assess_privacy_level(record)\n    privacy_score = 0\n    # Assess based on data sensitivity\n    IF contains_location_data(record) THEN\n        privacy_score += 30\n    END IF\n    IF contains_behavioral_data(record) THEN\n        privacy_score += 20\n    END IF\n    IF contains_demographic_data(record) THEN\n        privacy_score += 25\n    END IF\n    # Assess based on data granularity\n    IF high_temporal_resolution(record) THEN\n        privacy_score += 15\n    END IF\n    IF high_spatial_resolution(record) THEN\n        privacy_score += 10\n    END IF\n    RETURN classify_privacy_level(privacy_score)\nEND FUNCTION\n```\n## Utility Preservation\n```pseudocode\nFUNCTION optimize_utility_privacy_tradeoff(data, privacy_requirements, utility_requirements)\n    best_configuration = null\n    best_score = 0\n    FOR each configuration IN generate_anonymization_configurations()\n        anonymized_data = apply_anonymization(data, configuration)\n        privacy_score = evaluate_privacy_protection(anonymized_data)\n        utility_score = evaluate_data_utility(anonymized_data)\n        IF privacy_score >= privacy_requirements THEN\n            combined_score = calculate_combined_score(privacy_score, utility_score)\n            IF combined_score > best_score THEN\n                best_configuration = configuration\n                best_score = combined_score\n            END IF\n        END IF\n    END FOR\n    RETURN best_configuration\nEND FUNCTION\n```\n## Privacy Risk Assessment\n```pseudocode\nFUNCTION assess_privacy_risk(anonymized_data)\n    risk_factors = {}\n    # Assess re-identification risk\n    risk_factors.reidentification = assess_reidentification_risk(anonymized_data)\n    # Assess inference risk\n    risk_factors.inference = assess_inference_risk(anonymized_data)\n    # Assess linkage risk\n    risk_factors.linkage = assess_linkage_risk(anonymized_data)\n    # Calculate overall risk score\n    overall_risk = calculate_weighted_risk(risk_factors)\n    RETURN overall_risk\nEND FUNCTION\n```\n## Anonymization Quality Metrics\n```pseudocode\nFUNCTION calculate_anonymization_quality(original_data, anonymized_data)\n    quality_metrics = {}\n    # Privacy metrics\n    quality_metrics.k_anonymity_level = calculate_k_value(anonymized_data)\n    quality_metrics.l_diversity = calculate_l_diversity(anonymized_data)\n    quality_metrics.t_closeness = calculate_t_closeness(anonymized_data)\n    # Utility metrics\n    quality_metrics.information_loss = calculate_information_loss(original_data, anonymized_data)\n    quality_metrics.query_accuracy = evaluate_query_accuracy(original_data, anonymized_data)\n    # Compliance metrics\n    quality_metrics.gdpr_compliance = assess_gdpr_compliance(anonymized_data)\n    quality_metrics.differential_privacy_epsilon = calculate_epsilon_value(anonymized_data)\n    RETURN quality_metrics\nEND FUNCTION\n```\n## Advanced Privacy Techniques\n### Synthetic Data Generation\n```pseudocode\nFUNCTION generate_synthetic_data(original_data, privacy_budget)\n    # Learn statistical properties\n    statistical_model = learn_statistical_properties(original_data)\n    # Apply differential privacy to model parameters\n    private_model = apply_differential_privacy(statistical_model, privacy_budget)\n    # Generate synthetic records\n    synthetic_data = generate_records_from_model(private_model)\n    RETURN synthetic_data\nEND FUNCTION\n```\n### Federated Learning Integration\n```pseudocode\nFUNCTION federated_anonymization(distributed_data_sources)\n    global_anonymization_parameters = initialize_parameters()\n    FOR each iteration IN federated_training_iterations\n        local_updates = []\n        FOR each data_source IN distributed_data_sources\n            local_update = compute_local_anonymization_update(data_source, global_anonymization_parameters)\n            local_updates.append(local_update)\n        END FOR\n        # Aggregate updates with differential privacy\n        global_anonymization_parameters = aggregate_with_privacy(local_updates)\n    END FOR\n    RETURN global_anonymization_parameters\nEND FUNCTION\n```\n## Compliance Integration\nOur privacy and anonymization techniques are designed to meet:\n- **GDPR Requirements**: Right to be forgotten, data minimization, purpose limitation\n- **ePrivacy Directive**: Consent requirements, communication confidentiality\n- **Sector-Specific Regulations**: Telecommunications privacy requirements\n- **Eurostat Standards**: Statistical disclosure control requirements\nThe anonymization process integrates seamlessly with our [EU Compliance](./eu-compliance) methodology to ensure comprehensive regulatory adherence."
    },
    {
      "path": "methodology/quality-assurance.md",
      "content": "# Quality Assurance Methodology\nComprehensive validation methods and quality metrics to ensure data integrity throughout the Telcofy platform.\n## Quality Assurance Framework\nOur QA methodology encompasses four key dimensions:\n1. **Data Quality**: Accuracy, completeness, consistency, timeliness\n2. **Process Quality**: Pipeline reliability, performance consistency\n3. **Output Quality**: Result accuracy, statistical validity\n4. **Compliance Quality**: Regulatory adherence, audit readiness\n## Data Quality Metrics\n### Completeness Assessment\n```pseudocode\nFUNCTION assess_data_completeness(dataset)\n    completeness_metrics = {}\n    FOR each field IN dataset.required_fields\n        total_records = count_total_records(dataset)\n        non_null_records = count_non_null_records(dataset, field)\n        completeness_metrics[field] = {\n            completeness_ratio: non_null_records / total_records,\n            missing_count: total_records - non_null_records,\n            completeness_grade: classify_completeness(non_null_records / total_records)\n    END FOR\n    # Overall completeness score\n    completeness_metrics.overall_score = calculate_weighted_average(completeness_metrics)\n    RETURN completeness_metrics\nEND FUNCTION\n```\n### Accuracy Validation\n```pseudocode\nFUNCTION validate_data_accuracy(dataset, reference_data)\n    accuracy_metrics = {}\n    # Field-level accuracy\n    FOR each field IN dataset.fields\n        IF has_reference_data(field, reference_data) THEN\n            accuracy_metrics[field] = {\n                match_rate: calculate_match_rate(dataset[field], reference_data[field]),\n                error_rate: calculate_error_rate(dataset[field], reference_data[field]),\n                accuracy_score: calculate_accuracy_score(dataset[field], reference_data[field])\n        END IF\n    END FOR\n    # Business rule validation\n    accuracy_metrics.business_rules = validate_business_rules(dataset)\n    # Range validation\n    accuracy_metrics.range_validation = validate_field_ranges(dataset)\n    RETURN accuracy_metrics\nEND FUNCTION\n```\n### Consistency Checking\n```pseudocode\nFUNCTION check_data_consistency(dataset)\n    consistency_metrics = {}\n    # Internal consistency\n    consistency_metrics.internal = check_internal_consistency(dataset)\n    # Cross-field consistency\n    consistency_metrics.cross_field = check_cross_field_consistency(dataset)\n    # Temporal consistency\n    consistency_metrics.temporal = check_temporal_consistency(dataset)\n    # Format consistency\n    consistency_metrics.format = check_format_consistency(dataset)\n    # Calculate overall consistency score\n    consistency_metrics.overall_score = aggregate_consistency_scores(consistency_metrics)\n    RETURN consistency_metrics\nEND FUNCTION\n```\n## Process Quality Monitoring\n### Pipeline Performance Monitoring\n```pseudocode\nFUNCTION monitor_pipeline_performance()\n    performance_metrics = {}\n    # Throughput monitoring\n    performance_metrics.throughput = {\n        records_per_second: calculate_processing_rate(),\n        peak_throughput: get_peak_processing_rate(),\n        average_throughput: get_average_processing_rate()\n    # Latency monitoring\n    performance_metrics.latency = {\n        end_to_end_latency: measure_end_to_end_latency(),\n        stage_latencies: measure_stage_latencies(),\n        percentile_latencies: calculate_latency_percentiles()\n    # Error rate monitoring\n    performance_metrics.errors = {\n        error_rate: calculate_error_rate(),\n        error_types: categorize_errors(),\n        error_trends: analyze_error_trends()\n    # Resource utilization\n    performance_metrics.resources = {\n        cpu_utilization: measure_cpu_usage(),\n        memory_utilization: measure_memory_usage(),\n        storage_utilization: measure_storage_usage()\n    RETURN performance_metrics\nEND FUNCTION\n```\n### Data Lineage Tracking\n```pseudocode\nFUNCTION track_data_lineage(data_element)\n    lineage_info = {}\n    # Source tracking\n    lineage_info.source = {\n        original_source: identify_original_source(data_element),\n        ingestion_timestamp: get_ingestion_timestamp(data_element),\n        source_quality_score: get_source_quality_score(data_element)\n    # Transformation tracking\n    lineage_info.transformations = []\n    FOR each transformation IN get_applied_transformations(data_element)\n        transformation_info = {\n            transformation_type: transformation.type,\n            transformation_timestamp: transformation.timestamp,\n            transformation_parameters: transformation.parameters,\n            quality_impact: assess_transformation_quality_impact(transformation)\n        lineage_info.transformations.append(transformation_info)\n    END FOR\n    # Output tracking\n    lineage_info.outputs = track_data_outputs(data_element)\n    RETURN lineage_info\nEND FUNCTION\n```\n## Output Quality Validation\n### Statistical Validity Checks\n```pseudocode\nFUNCTION validate_statistical_output(statistical_results)\n    validity_checks = {}\n    # Distribution validation\n    validity_checks.distribution = {\n        normality_test: test_normality(statistical_results),\n        outlier_detection: detect_statistical_outliers(statistical_results),\n        range_validation: validate_statistical_ranges(statistical_results)\n    # Correlation validation\n    validity_checks.correlation = {\n        expected_correlations: validate_expected_correlations(statistical_results),\n        spurious_correlations: detect_spurious_correlations(statistical_results)\n    # Temporal validation\n    validity_checks.temporal = {\n        trend_consistency: validate_trend_consistency(statistical_results),\n        seasonal_patterns: validate_seasonal_patterns(statistical_results)\n    # Statistical significance\n    validity_checks.significance = {\n        confidence_levels: calculate_confidence_levels(statistical_results),\n        p_values: calculate_p_values(statistical_results),\n        effect_sizes: calculate_effect_sizes(statistical_results)\n    RETURN validity_checks\nEND FUNCTION\n```\n### Cross-Validation Framework\n```pseudocode\nFUNCTION perform_cross_validation(model, dataset)\n    cv_results = {}\n    # K-fold cross-validation\n    cv_results.k_fold = perform_k_fold_validation(model, dataset, k=5)\n    # Time series cross-validation\n    IF is_time_series_data(dataset) THEN\n        cv_results.time_series = perform_time_series_validation(model, dataset)\n    END IF\n    # Stratified cross-validation\n    IF has_categorical_target(dataset) THEN\n        cv_results.stratified = perform_stratified_validation(model, dataset)\n    END IF\n    # Calculate cross-validation metrics\n    cv_results.metrics = {\n        mean_accuracy: calculate_mean_cv_accuracy(cv_results),\n        std_accuracy: calculate_std_cv_accuracy(cv_results),\n        confidence_interval: calculate_cv_confidence_interval(cv_results)\n    RETURN cv_results\nEND FUNCTION\n```\n## Compliance Quality Assurance\n### Regulatory Compliance Validation\n```pseudocode\nFUNCTION validate_regulatory_compliance(processed_data)\n    compliance_validation = {}\n    # GDPR compliance checks\n    compliance_validation.gdpr = {\n        data_minimization: validate_data_minimization(processed_data),\n        purpose_limitation: validate_purpose_limitation(processed_data),\n        storage_limitation: validate_storage_limitation(processed_data),\n        consent_compliance: validate_consent_requirements(processed_data)\n    # Eurostat compliance checks\n    compliance_validation.eurostat = {\n        classification_compliance: validate_classification_standards(processed_data),\n        quality_standards: validate_eurostat_quality_standards(processed_data),\n        disclosure_control: validate_disclosure_control(processed_data)\n    # Privacy compliance checks\n    compliance_validation.privacy = {\n        anonymization_quality: validate_anonymization_quality(processed_data),\n        re_identification_risk: assess_re_identification_risk(processed_data),\n        privacy_utility_tradeoff: assess_privacy_utility_balance(processed_data)\n    RETURN compliance_validation\nEND FUNCTION\n```\n### Audit Trail Validation\n```pseudocode\nFUNCTION validate_audit_trail(audit_logs)\n    audit_validation = {}\n    # Completeness of audit logs\n    audit_validation.completeness = {\n        log_coverage: assess_log_coverage(audit_logs),\n        missing_events: identify_missing_audit_events(audit_logs),\n        log_retention: validate_log_retention_compliance(audit_logs)\n    # Integrity of audit logs\n    audit_validation.integrity = {\n        log_integrity: validate_log_integrity(audit_logs),\n        tamper_evidence: check_tamper_evidence(audit_logs),\n        chronological_order: validate_chronological_order(audit_logs)\n    # Accessibility of audit logs\n    audit_validation.accessibility = {\n        search_capability: validate_log_search_capability(audit_logs),\n        export_capability: validate_log_export_capability(audit_logs),\n        reporting_capability: validate_reporting_capability(audit_logs)\n    RETURN audit_validation\nEND FUNCTION\n```\n## Automated Quality Monitoring\n### Real-Time Quality Alerts\n```pseudocode\nFUNCTION monitor_quality_real_time(data_stream)\n    quality_alerts = []\n    FOR each data_batch IN data_stream\n        # Real-time quality checks\n        quality_scores = calculate_real_time_quality_scores(data_batch)\n        # Check against thresholds\n        FOR each metric IN quality_scores\n            IF quality_scores[metric] < QUALITY_THRESHOLDS[metric] THEN\n                alert = {\n                    alert_type: \"quality_degradation\",\n                    metric: metric,\n                    current_value: quality_scores[metric],\n                    threshold: QUALITY_THRESHOLDS[metric],\n                    timestamp: current_timestamp(),\n                    severity: determine_alert_severity(quality_scores[metric], QUALITY_THRESHOLDS[metric])\n                quality_alerts.append(alert)\n                trigger_quality_alert(alert)\n            END IF\n        END FOR\n    END FOR\n    RETURN quality_alerts\nEND FUNCTION\n```\n### Quality Trend Analysis\n```pseudocode\nFUNCTION analyze_quality_trends(quality_history)\n    trend_analysis = {}\n    # Temporal trend analysis\n    trend_analysis.temporal_trends = {\n        daily_trends: analyze_daily_quality_trends(quality_history),\n        weekly_trends: analyze_weekly_quality_trends(quality_history),\n        monthly_trends: analyze_monthly_quality_trends(quality_history)\n    # Quality degradation prediction\n    trend_analysis.predictions = {\n        predicted_quality: predict_future_quality(quality_history),\n        degradation_risk: assess_degradation_risk(quality_history),\n        intervention_recommendations: recommend_quality_interventions(quality_history)\n    # Root cause analysis\n    trend_analysis.root_causes = {\n        correlation_analysis: analyze_quality_correlations(quality_history),\n        anomaly_sources: identify_quality_anomaly_sources(quality_history),\n        process_impacts: assess_process_quality_impacts(quality_history)\n    RETURN trend_analysis\nEND FUNCTION\n```\n## Quality Reporting and Dashboards\n### Quality Scorecard Generation\n```pseudocode\nFUNCTION generate_quality_scorecard(time_period)\n    scorecard = {}\n    # Data quality scores\n    scorecard.data_quality = {\n        overall_score: calculate_overall_data_quality_score(time_period),\n        completeness_score: calculate_completeness_score(time_period),\n        accuracy_score: calculate_accuracy_score(time_period),\n        consistency_score: calculate_consistency_score(time_period),\n        timeliness_score: calculate_timeliness_score(time_period)\n    # Process quality scores\n    scorecard.process_quality = {\n        pipeline_reliability: calculate_pipeline_reliability(time_period),\n        performance_consistency: calculate_performance_consistency(time_period),\n        error_recovery: calculate_error_recovery_efficiency(time_period)\n    # Compliance scores\n    scorecard.compliance_quality = {\n        regulatory_compliance: calculate_regulatory_compliance_score(time_period),\n        privacy_compliance: calculate_privacy_compliance_score(time_period),\n        audit_readiness: calculate_audit_readiness_score(time_period)\n    # Generate recommendations\n    scorecard.recommendations = generate_quality_improvement_recommendations(scorecard)\n    RETURN scorecard\nEND FUNCTION\n```\nThis quality assurance methodology ensures that Telcofy maintains the highest standards of data integrity, process reliability, and compliance throughout the entire analytics lifecycle."
    },
    {
      "path": "use-cases/emergency-response.md",
      "content": "# Emergency Response\nComing soon..."
    },
    {
      "path": "use-cases/eurostat.md",
      "content": "# Eurostat Reporting\nComing soon..."
    },
    {
      "path": "use-cases/tourism.md",
      "content": "# Tourism Analytics\nComing soon..."
    },
    {
      "path": "use-cases/urban-planning.md",
      "content": "# Urban Planning\nComing soon..."
    },
    {
      "path": "architecture/data-pipeline.md",
      "content": "# Data Pipeline\nComing soon..."
    },
    {
      "path": "architecture/overview.md",
      "content": "# Architecture Overview\nComing soon..."
    },
    {
      "path": "architecture/scalability.md",
      "content": "# Scalability\nComing soon..."
    },
    {
      "path": "architecture/security.md",
      "content": "# Security\nComing soon..."
    },
    {
      "path": "support/contact.md",
      "content": "# Contact Support\nComing soon..."
    },
    {
      "path": "support/faq.md",
      "content": "# FAQ\nComing soon..."
    },
    {
      "path": "support/troubleshooting.md",
      "content": "# Troubleshooting\nComing soon..."
    },
    {
      "path": "eu-compliance/codebase-overview.md",
      "content": "# MultiMNO Reference Codebase\nThe open-source **MultiMNO** repository ([GitHub](https://github.com/eurostat/multimno)) implements the end-to-end pipeline mandated by Eurostat. Telcofy mirrors this structure so that every processing step, quality artefact, and configuration can be mapped back to the ESS reference.\n## Architectural building blocks\n- **Component-based Spark jobs** â€” each processing step subclasses a common `Component`, standardising configuration parsing, Spark session management, and IO (`multimno/core/component.py`).\n- **Config-driven orchestration** â€” `orchestrator_multimno.py` reads a JSON pipeline definition and launches each component via `spark-submit`, capturing exit codes that distinguish success, warnings, or errors.\n- **Reusable data objects** â€” bronze (raw), silver (curated), and gold (publishable) layers are represented by typed IO wrappers (`multimno/core/data_objects`). Schema definitions, partitioning strategies, and helper methods (e.g., size, head, partition count) keep quality checks consistent.\n## Lifecycle at a glance\n1. **Setup / reference data**  \n   Generate INSPIRE grid tiles, geo-enrichment tables, and auxiliary mappings (holidays, MCC â†’ ISO, time zones).\n2. **Daily ingestion & cleaning**  \n   - Ingest raw network topology and event streams.  \n   - Apply syntactic + semantic cleaning modules plus device-level quality metrics.\n3. **Coverage modelling**  \n   - Estimate signal strength, cell footprints, and connection probabilities (Modulesâ€¯3â€“5).  \n   - Produce the grid-to-cell probabilities consumed by analytical modules.\n4. **Daily analytics**  \n   - Present Population estimation.  \n   - Continuous Time Segmentation (CTS).  \n   - Daily Permanence Score (DPS).\n5. **Longitudinal analytics**  \n   - Mid-term and long-term permanence scoring.  \n   - Usual Environment, Home Location, Internal Migration.  \n   - Tourism (inbound/outbound) indicators.\n6. **Aggregation & delivery**  \n   - Secure fusion across MNOs.  \n   - Projection to target geographies.  \n   - Estimation/calibration plus Statistical Disclosure Control (SDC).  \n   - Publishing of gold datasets and quality dashboards.\n## Configuration hierarchy\n- `pipe_configs/pipelines/*.json` â€” execution graphs with ordered component IDs.\n- `pipe_configs/configurations/general_config.ini` â€” shared Spark, IO, logging, and temp-path settings.\n- Component-specific INIs â€” override or extend per-module parameters (e.g., tolerances, pruning thresholds, quality gates).\n- Sample data (`sample_data/lakehouse`) â€” synthetic lakehouse for local validation.\n## Supporting assets\n- **Documentation** (`docs/`) â€” system requirements, pipeline diagrams, developer guides.\n- **Deployment** (`deployment/`) â€” Dockerfiles and scripts for containerised execution.\n- **Tests** (`tests/`) â€” unit and integration tests aligning with Eurostatâ€™s quality modules.\n## Telcofy checklist\n- Keep component IDs and configuration keys unchanged so regulators can diff our pipelines against MultiMNO.\n- Persist orchestrator logs and module output paths; they provide the audit trail for ESS quality reviews.\n- When extending components, add new configuration options rather than modifying defaults silently. Document overrides in Telcofyâ€™s run books."
    },
    {
      "path": "eu-compliance/eurostat-pipeline-staypoint-detection.md",
      "content": "# Staypoint Detection in the MultiMNO Pipeline\nEurostatâ€™s MultiMNO reference implementation detects staypointsâ€”periods when a subscriber is stationary in a consistent locationâ€”by combining *Present Population* estimation with the *Continuous Time Segmentation* module. Together these stages transform raw Mobile Network Operator (MNO) events into longitudinal insights that underpin official statistics on residence, mobility, and tourism. Telcofy reuses the same logic as documented in the open-source repository on [GitHub](https://github.com/eurostat/multimno).\n## Input data and prerequisite layers\nStaypoint detection builds upon the **silver** layer generated earlier in the pipeline:\n- `SilverEventFlaggedDataObject`: cleaned usage events annotated with semantic quality flags and network metadata.\n- `SilverCellConnectionProbabilitiesDataObject`: grid-to-cell probabilities derived from radio coverage and calibration routines.\n- `SilverCellIntersectionGroupsDataObject`: for each day, the list of neighbouring cells that form a consistent spatial footprint.\n- `EventCacheDataObject`: cached â€œfirst events of the following dayâ€ that allow segments to connect across date boundaries.\nThese datasets are materialised by upstream components such as Event Cleaning, Semantic Cleaning, and Cell Footprint Estimation (see [Methodological Framework](./methodological-framework.md) for the full overview).\n## Present Population: probabilistic occupancy counts\nThe **Present Population** component (`present_population_estimation.py`) calculates occupancy per grid tile and timestamp. Although its main output is population estimates, the method establishes the temporal window that later feeds staypoint logic:\n1. **Event windowing** â€“ for each query timestamp, the code keeps events within a configurable tolerance window (`tolerance_period_s`) and selects the event closest to the time point per device (Window spec sorted by time distance, see `calculate_devices_per_cell`).\n2. **Device-to-grid weighting** â€“ device counts per cell are merged with cell-to-grid probabilities and iterated through a Bayesian normalisation loop until differences fall below `min_difference_threshold`.\n3. **Persisted cache** â€“ intermediate population distributions are written to `pp_cache` to track convergence and supply stable inputs to downstream modules.\nThis step ensures that when a device remains within one cell (or cluster of overlapping cells) across multiple time points, the occupancy signal remains coherent, a critical prerequisite for consistent staypoint labelling.\n## Continuous Time Segmentation: stay, move, abroad, unknown\nThe **Continuous Time Segmentation** component (`continuous_time_segmentation.py`) converts daily batches of events into time segments tagged with behavioural states. Key stages include:\n1. **Daily slicing with cross-day continuity**  \n   For each date in `[data_period_start, data_period_end]`, events are filtered by error flags and domains, then unioned with the first event of the next day (via `EventCacheDataObject`). This ensures that a stay spanning midnight is not split prematurely.\n2. **Context enrichment**  \n   Events are joined with intersection groups to recover all overlapping cells, enabling spatial buffering when multiple towers cover the same location. If segments from the previous day exist, they are joined back to preserve ending state and cell set.\n3. **Segmentation parameters**  \n   Configuration knobs (defined in `continuous_time_segmentation.ini`) drive the classification logic:\n   - `min_time_stay_s`: minimum dwell time for a *STAY* segment (default 900s).\n   - `max_time_missing_stay_s`, `max_time_missing_move_s`, `max_time_missing_abroad_s`: largest gap tolerated between events before inserting an `UNKNOWN` state.\n   - `pad_time_s`: slight temporal expansion applied to isolated segments between `UNKNOWN` blocks.\n   - `domains_to_include`: inbound, domestic, outbound domains processed; outbound-only runs still depend on domestic events to close segments.\n   - `local_mcc`: identifies the home country, used to classify ABROAD segments.\n4. **User-level UDF**  \n   Events per user are grouped and fed to a Pandas UDF (`segmentation_return_schema`). The UDF tracks sequences of events, extends segments while devices remain within overlapping cells, and creates new segments when:\n   - the device crosses domain boundaries (e.g., domestic â†’ outbound),\n   - the time gap exceeds the relevant `max_time_missing_*` threshold,\n   - or overlapping cells no longer intersect.\n5. **State assignment**  \n   Each segment receives one of four enumerated states (`SegmentStates`):\n   - `STAY` â€“ dwell longer than `min_time_stay`.\n   - `MOVE` â€“ transitions with sufficient evidence of displacement.\n   - `ABROAD` â€“ activity outside the local MCC.\n   - `UNKNOWN` â€“ gaps or ambiguous signals beyond configured tolerances.\n6. **Persistence with continuity**  \n   Segments are written to the silver layer (`time_segments_silver`) with `is_last` markers indicating the trailing segment per user/day. Subsequent days reuse these markers to carry context forward.\n## Outputs and downstream usage\n- **Staypoint table** (`time_segments_silver`): each record contains start/end timestamps, last observed event, list of contributing cells, state, and a hashed `time_segment_id`. This dataset powers Midterm/Longterm Permanence calculations, usual environment labelling, and tourism metrics.\n- **Population snapshots**: the `present_population_silver` table is refreshed for each analysed timestamp and feeds daily dashboards as well as the iterative permanence scores.\nTelcofy extends these outputs with add-on analytics (e.g., persistence scoring, anomaly flagging) while preserving the upstream logic to remain interoperable with Eurostat deliverables.\n## Telcofy alignment checklist\n- âœ… **Reuse** the Continuous Time Segmentation configuration structure so auditors can compare parameters directly with the MultiMNO defaults.\n- âœ… **Preserve** the four-state taxonomy and `is_last` markers, even when enriching segments with Telcofy-specific metadata.\n- âœ… **Document** any changes to tolerance windows or dwell thresholds and justify them against ESS quality guidance.\n- âœ… **Propagate** population caches and time segments into Telcofyâ€™s compliance artifacts so EU stakeholders can trace staypoint derivations end-to-end."
    },
    {
      "path": "eu-compliance/index.md",
      "content": "# Multi-MNO Compliance Hub\nEurostatâ€™s **Multi-MNO** programme (service contract ESTATâ€¯2021.0400) delivers the standards Telcofy follows when producing EU-grade mobility analytics. The project combines: (1) a methodological blueprint, (2) an open-source PySpark implementation, and (3) a catalogue of statistical products for National Statistical Institutes (NSIs). This documentation cluster distils those artefacts and records how Telcofy aligns with them.\n## How to navigate this section\n- [Methodological Framework (Vol.â€¯I)](./methodological-framework.md) â€” key principles, reference vs. demonstrator scenarios, and design rules for the pipeline.\n- [Use Cases (Vol.â€¯II)](./use-cases.md) â€” the seven statistical products (present population, usual environment, tourism, etc.) derived from MNO data.\n- [Methods & Data Objects (Vol.â€¯III)](./methods.md) â€” detailed algorithms, equations, and intermediate datasets underlying each module.\n- [Codebase & Orchestration](./codebase-overview.md) â€” how the open-source repository is structured and how Telcofy mirrors it operationally.\n- [Quality Framework](./quality.md) â€” business processes, input/output QC, and reporting expectations for MNO-based statistics.\n- [Staypoint Detection](./eurostat-pipeline-staypoint-detection.md) â€” deep dive on Continuous Time Segmentation and related staypoint logic.\n## Programme snapshot\n- **Timeline** â€” Contract awarded Januaryâ€¯2023, concluding midâ€‘2025 after multi-country pilots with five participating MNOs.\n- **Governance** â€” Driven by Eurostatâ€™s Directorate ESTAT.A.5 with oversight from the ESS Task Force on MNO data and an external Advisory Board.\n- **Resources** â€” Public (landing page)[https://cros.ec.europa.eu/landing-page/multi-mno-project] with deliverables and materials.\n- **Tooling** â€” Open-source code [D4.4](https://cros.ec.europa.eu/group/6/files/2659/download) released in 2025 (version 1.0) ; ongoing quality framework updates (D3.3)(https://cros.ec.europa.eu/group/6/files/2658/download) and testing reports (D5.2)[https://cros.ec.europa.eu/group/6/files/2661/download].\n## Why it matters for Telcofy\n- **Regulatory assurance** â€” Adhering to the MultiMNO modules lets Telcofy evidence privacy, quality, and governance controls when onboarding EU customers.\n- **Interoperable outputs** â€” Matching USE case definitions and data schemas ensures our indicators can be benchmarked against ESS publications.\n- **Future-proofing** â€” Tracking Eurostat revisions keeps our data-sharing agreements and infrastructure aligned with evolving EU legislation (e.g., Regulation (EU)â€¯2024/3018, amendments to Regulation (EC)â€¯Noâ€¯223/2009).\n## Privacy and data access posture\n- **Privacy-by-design** â€” Following guidance from [MNOdata4OS](https://cros.ec.europa.eu/MNOdata4OS), Telcofy restricts raw-event handling to secure computation environments, enforces staged aggregation, and retains full provenance/retention logs.\n- **Sustainable access models** â€” Contracts mirror the MultiMNO reference scenario: explicit mandates for MNO-to-NSI sharing, cost-recovery mechanisms, and options for secure enclaves when cross-MNO fusion is required.\n## Deliverables at a glance\nThe full documentation set is available via Eurostatâ€™s portal: [D2.3 â€“ Methodology, requirements, use cases and methods](https://cros.ec.europa.eu/book-page/methodology-framework-high-level-architecture-requirements-use-cases-and-methods).\n- **D2.3 Vol.â€¯I** â€” Conceptual & methodological framework ([download](https://cros.ec.europa.eu/group/6/files/2655/download)); see [Methodological Framework](./methodological-framework.md).\n- **D2.3 Vol.â€¯II** â€” Statistical use cases ([download](https://cros.ec.europa.eu/group/6/files/2656/download)); see [Use Cases](./use-cases.md).\n- **D2.3 Vol.â€¯III** â€” Methods & data objects ([download](https://cros.ec.europa.eu/group/6/files/2657/download)); see [Methods & Data Objects](./methods.md).\n- **D3.3** â€” Business process & quality frameworxk ([download](https://cros.ec.europa.eu/group/6/files/2658/download)); Telcofyâ€™s adoption is summarised in [Quality Framework](./quality.md).\n- **D4.4** â€” Software releases & documentation ([download](https://cros.ec.europa.eu/group/6/files/2659/download)); reflected in [Codebase & Orchestration](./codebase-overview.md).\nUse the linked pages to dive deeper into each aspect. Together they provide the traceability auditors expect when validating Telcofyâ€™s EU compliance posture."
    },
    {
      "path": "eu-compliance/methodological-framework.md",
      "content": "# MultiMNO Methodological Framework Summary\nDeliverable **[D2.3 Volume I](https://cros.ec.europa.eu/group/6/files/2655/download)** of the MultiMNO project (â€œDevelopment, implementation and demonstration of a reference processing pipeline for the future production of official statistics based on multiple MNO dataâ€) codifies the conceptual standard that Telcofy aligns with when designing compliant EU products. This note distils the key elements relevant for our engineering teams. (See also the [CROS deliverables overview](https://cros.ec.europa.eu/book-page/methodology-framework-high-level-architecture-requirements-use-cases-and-methods) for context.)\n## Purpose and scope\n- Establishes a reference processing pipeline that the European Statistical System (ESS) can adopt for population presence and mobility statistics derived from multiple operators.\n- Couples a methodological blueprint with an open-source implementation, ensuring evolvability across new use cases beyond the six piloted by the project.\n- Focuses on conceptual and semantic layers; syntactic conventions and software design are detailed in other deliverables but remain consistent with this framework.\n## High-level requirements\n- **Methodological soundness** by incorporating state-of-the-art techniques while remaining open to future advancements.\n- **Integration of prior ESS work** (e.g., ESSnet Big Data, TF-MNO) to avoid reinventing earlier findings yet replace obsolete approaches when necessary.\n- **Structured consultation** with the Advisory Board and ESS domain experts to validate feasibility and statistical rigor.\n- **Evolvability and modularity** so components can be swapped, extended, or re-parameterised without redesigning the entire pipeline.\n- **Quality assurance and explainability** embedded at every stage, combining automated checks, human review triggers, and auditable logs.\n## Reference scenario\n- Assumes legally enabled access to data from *all* significant MNOs in each Member State, with harmonised privacy safeguards and sustainable business models.\n- Distinguishes between reference (target future state) and demonstrator (current legal constraints) scenarios; Telcofy must be ready to operate in either.\n- Mandates processing on a standard spatial grid (INSPIRE) and highlights the need for secure, privacy-preserving data handling whether on MNO premises or in trusted shared environments.\n## Fundamental design principles\n1. **Common spatial grid** â€“ everything aligns to a shared INSPIRE grid; projections to administrative units happen downstream with standardised methods.\n2. **Multiscale longitudinal analysis** â€“ three stacked time horizons (daily, mid-term, long-term) progressively reduce data granularity while honouring GDPR minimisation.\n3. **Input accessibility** â€“ any module may *read* upstream data objects, but only designated modules may *write* to maintain deterministic flows.\n4. **Bottom-up one-way processing** â€“ higher-level summaries never rewrite lower-level artefacts, preventing retrospective bias.\n5. **Separated integration dimensions** â€“ temporal, spatial, and unit aggregations are handled in distinct modules wherever possible.\n6. **Balance flexibility vs. parsimony** â€“ allow multiple method variants when justified by different use cases, but avoid proliferating redundant alternatives.\n7. **Soft classification & uncertainty management** â€“ promote probabilistic labels and confidence tracking, facilitating later reweighting or disclosure control.\n## Pipeline overview\nThe methodology decomposes the processing flow into six macro stages (Figure references in the deliverable):\n1. **Network topology ingestion (syntactic checks + spatial enrichment)**  \n   Standardises cell identifiers, grid mappings, and overlapping-cell groups to support coverage modelling.\n2. **Event data processing**  \n   - Syntactic cleansing of raw events (CDRs, IPDRs) followed by semantic validation (e.g., impossible jumps, non-existent cells).  \n   - Generates device-level quality metrics to flag sparse or anomalous behaviour.\n3. **Multi-scale longitudinal analysis per device**  \n   - Daily processing (e.g., Continuous Time Segmentation, Daily Permanence Score) yields â€œdaily summariesâ€.  \n   - Mid-term modules aggregate daily outputs (monthly/quarterly) before long-term labelling (usual environment, residence).  \n   - Modular structure enables parallel methods tailored to different use cases (snapshot vs. longitudinal).\n4. **Indicator calculation per use case**  \n   - Recombines daily/mid-term/long-term summaries with configuration metadata to produce thematic indicators (present population, tourism, internal migration, etc.).  \n   - Supports alternative inference strategies (direct totals, rescaling, weighting) while tracking the statistical population used per indicator.\n5. **Multi-MNO data fusion and transfer**  \n   - Defines both a *basic* approach (single-MNO aggregates, SDC applied before leaving premises) and an *advanced* approach (secure enclave enabling individual-level fusion of summaries).  \n   - Aligns responsibilities between MNOs, NSIs, and potential trusted third parties.\n6. **Statistical Disclosure Control (SDC) and quality checks**  \n   - Applies output SDC and harmonised quality reporting prior to dissemination.  \n   - Maintains audit trails that demonstrate compliance with privacy regulations and ESS quality frameworks.\n## Supporting assets\n- **Glossary** (Annex 2) ensures a shared vocabulary across NSIs and industry partnersâ€”critical when interpreting Telcofy deliverables.\n- **Annex on project tasks** maps methodological design (Task 2) to software development (Task 4), clarifying where Telcofy should plug in enhancements.\n## Telcofy alignment notes\n- Maintain the bronze/silver/gold layering and INSPIRE grid usage so Telcofy outputs remain interoperable with ESS artefacts.\n- When extending or substituting modules (e.g., alternative staypoint algorithms), document decisions against the principles above and retain explainability metadata.\n- Respect the bottom-up one-way processing ruleâ€”especially when introducing Telcofyâ€™s add-ons for forecasting or anomaly detectionâ€”to avoid retroactive modification of lower-level summaries.\n- Prepare for both basic and advanced fusion setups: ensure our contractual and technical controls can operate when only aggregated data leave MNO premises or when secure enclaves become available.\nRefer to the full deliverable [D2.3 Vol.â€¯I](https://cros.ec.europa.eu/group/6/files/2655/download) for diagrammatic details and the exhaustive specification of objects, functions, and quality rules."
    },
    {
      "path": "eu-compliance/methods.md",
      "content": "# Core Methods and Data Objects\nEurostatâ€™s Deliverable **[D2.3 Volume III](https://cros.ec.europa.eu/group/6/files/2657/download)** documents the algorithms, quality checks, and data contracts that make up the MultiMNO reference pipeline. The volume specifies, in detail, the full set of methods and data objects developed for four core use cases: Present Population Estimation, M-Usual Environment Indicators, M-Home Location Indicators, and Internal Migration. These methods sit underneath the framework (Vol.â€¯I) and use case catalogue (Vol.â€¯II) already summarised in this section of the docs. Telcofy keeps the same modular structure so that regulators can trace every output back to an ESS-aligned procedure. (Additional background is available via the [CROS deliverables overview](https://cros.ec.europa.eu/book-page/methodology-framework-high-level-architecture-requirements-use-cases-and-methods).)\n- *M-Home Location Indicators*: derived from long-term permanence outputs, this pipeline labels each deviceâ€™s de facto residence, aggregates counts to grids or administrative areas, and attaches quality/confidence scores plus change flags so downstream processes (e.g., internal migration) can quantify relocations.\n## 1. Network topology & coverage modules\n### Module 1 â€” MNO network topology data cleaning (syntactic checks)\nThe first module scrubs raw topology feeds (cell plan CSV, JSON, XML) into the canonical schema. Validation falls\ninto three buckets:\n- **Coordinate hygiene** â€” latitude/longitude must exist, be parseable as WGSâ€¯84, and fall inside the bounding\n  geometry provided for the target country. Optional altitude and antenna height are checked against admissible\n  ranges (e.g. metres above sea level).\n- **Antenna descriptors** â€” directionality, azimuth, elevation, power, beam widths, frequency, technology, and\n  cell type are individually verified for presence (if required), data type, and inclusion in the enumerated value\n  sets delivered with D2.3.\n- **Geometry payloads** â€” when MNOs supply polygons (`Cell Footprint with Differentiated Signal Strength\n  Coverage Areas`), Well Known Text is parsed and clipped to the bounding box; signal strength attributes must\n  lie in the interval (0,â€¯1].\nRows failing mandatory-field checks are dropped; optional-field failures are nulled but logged. The method emits:\n- `Clean MNO Network Topology Data` â€” the sanitised dataset with type-correct columns ready for downstream\n  geospatial joins.\n- `MNO Network Topology Data Quality Metrics` â€” counts of rows removed per validation rule plus the k most\n  frequent error signatures, so recurring issues can be negotiated with the operator.\n### Module 2 â€” MNO network topology quality warnings\nModuleâ€¯2 turns the metrics above into actionable warnings. For each topology refresh (typically daily), key series\nsuch as *raw row count*, *post-clean row count*, and *field-level error ratios* are tracked and compared with historical\nbehaviour over a configurable look-back window (week, month, or quarter). Threshold types include:\n- **Absolute** â€” e.g. â€œraise a warning if the number of cells falls below 10â€¯000â€.\n- **Relative** â€” e.g. â€œraise a warning if todayâ€™s error rate exceeds the trailing mean by 30â€¯%â€.\n- **Control limits** â€” control-chart style bands computed as `Î¼ Â± kÂ·Ïƒ`.\nWarnings are grouped by metric (size, missing values, wrong format, out-of-range, parsing errors) and the output\ndashboard plots the time series with annotated breaches so analysts can distinguish genuine network changes\nfrom ingest glitches. All thresholds can be tuned per NSI; defaults follow the values published in D2.3.\n### Module 3 â€” Signal strength estimation\nWhen only the radio plan is available, Moduleâ€¯3 synthesises a radio-frequency coverage surface. Two variants are\ndocumented in D2.3; Telcofy implements both.\n#### 3.1 Omnidirectional model\nFor small cells or sectors with no directional beam, the received signal strength for cell `a` at grid tile `g` is\n![Omnidirectional model](/img/eu-compliance/omnidirectional-model.png)\nwhere:\n- `Sâ‚€` is the transmit power expressed as signal strength at reference distance `râ‚€ = 1â€¯m` (converted from the\n  cell plan power `P` in Watts via the standard Watt â†”â€¯dBm conversion).\n- `r_{g,a}` is the 3â€‘D distance between the antenna position and the tile centroid (assuming user devices at ground level).\n- `S_{\\text{dist}}(r)` encodes the distance-based attenuation:\nwith path-loss exponent `Î³` describing reflections, diffraction, and clutter (default Î³â€¯=â€¯2 for free space, configurable for urban/rural scenes).\n#### 3.2 Directional model\nMacrocells add angular attenuation. Let `Ï†_a` be the azimuth, `Î¸_a` the downtilt, `Î±_a` the horizontal 3â€¯dB beam\nwidth, and `Î²_a` the vertical 3â€¯dB width. Define:\n- `Î´_{g,a}` â€” difference between `Ï†_a` and the azimuth from cell `a` to tile `g`.\n- `Îµ_{g,a}` â€” elevation offset between `Î¸_a` and the line of sight to tile `g`.\nThe directional signal model becomes\n![Directional model](/img/eu-compliance/directional-model.png)\nwhere `S_{\\text{azi}}` and `S_{\\text{elev}}` are Gaussian-shaped attenuation functions calibrated so that the loss\nreaches 3â€¯dB at `Ï†_a Â± Î±_a/2` and `Î¸_a Â± Î²_a/2`, respectively. D2.3 derives these functions by solving for the\nparameters `c` and `Ïƒ` in\n![Gaussian attenuation curve](/img/eu-compliance/gaussian-formula.png)\nsubject to the 3â€¯dB constraints for each plane.\nFigureâ€¯2 summarises the resulting radiation pattern: the main lobe is centred on the beam direction, grey rings\nindicate successive 5â€¯dB loss levels, and the red arms mark the 3â€¯dB beam limits (spanning `2Î±_a` horizontally and\n`2Î²_a` vertically). Side or back lobes may arise in real hardware, but the Gaussian attenuation captures the\ndominant behaviour used for downstream modelling.\n![Radiation patterns for azimuth and elevation planes](/img/eu-compliance/methods-module3-fig2.png)\n*Source: MultiMNO D2.3 Vol.â€¯III, Figureâ€¯2.*\nOnce `S_{g,a}` is known, the coverage surface is rasterised onto the project grid by intersecting the continuous\nfootprint with target tilesâ€”see Figuresâ€¯3â€“5 for the projection workflow extracted from D2.3. Figureâ€¯4, in\nparticular, shows how arbitrary coverage shapes are snapped to the `project_grid_cell` lattice by attributing a\nspecific strength (or other cell property) to every intersected tile.\n![Example directional coverage on the ground plane](/img/eu-compliance/methods-module3-fig3.png)\n*Source: MultiMNO D2.3 Vol.â€¯III, Figureâ€¯3.* Here the cell (located at xâ€¯=â€¯0, yâ€¯=â€¯0, heightâ€¯=â€¯55â€¯m, Î³â€¯=â€¯4, powerâ€¯=â€¯10â€¯W,\nazimuth pointing east, tiltâ€¯=â€¯5Â°, horizontal widthâ€¯=â€¯65Â°, vertical widthâ€¯=â€¯9Â°) produces peak signal a few hundred\nmetres from the mast because tiles directly underneath have larger elevation offsets `Îµ_{g,a}`.\n![Rasterisation of coverage polygons to the project grid](/img/eu-compliance/methods-module3-fig4.png)\n*Source: MultiMNO D2.3 Vol.â€¯III, Figureâ€¯4.*\n![Intersection of MNO tiles with project tiles (Equationâ€¯1 in D2.3)](/img/eu-compliance/methods-module3-fig5.png)\n*Source: MultiMNO D2.3 Vol.â€¯III, Figureâ€¯5.* The graphic demonstrates Equationâ€¯1 (below): intersect each MNO tile\nwith the `project_grid_cell`, compute the overlap area `A_{ij}`, and aggregate the MNO strengths `SE_j` into the\nproject tile `i`.\n#### 3.3 Precomputed signal-strength tiles\nWhen the operator already supplies gridded signal strengths, Moduleâ€¯3 reprojects them. For each project tile `i`,\nintersecting with `n` MNO tiles, the blended signal is obtained using Equationâ€¯1 (below).\n![Equation 1](/img/eu-compliance/equation-1.png)\nHere `A_{ij}` is the overlap area between project tile `i` and MNO tile `j`, `A_T` is the area of tile `i`, and `S_j`\nis the strength attached to MNO tile `j`.\n### Module 4 â€” Cell footprint estimation\nSignal strengths are converted to dimensionless *footprint* values `s(g,a)` in the range `[0,1]`, representing a\ncellâ€™s relative suitability at tile `g`. D2.3 offers two transformation families:\n- **Linear**\n![Linear transformation illustration](/img/eu-compliance/linear-transformation.png)\nwith default `S_{\\min} = -130â€¯\\text{dBm}` and `S_{\\max} = -50â€¯\\text{dBm}`.\n- **Logistic**\n![Logistic transformation illustration](/img/eu-compliance/logistic-transformation.png)\nmirroring the â€œsignal dominanceâ€ concept in Tennekes & Gootzen (2022). Parameters `S_{\\text{steep}}` and\n`S_{\\text{mid}}` control the knee of the curve; defaults `(-92.5,\\;0.2)` capture typical macro-cell behaviour.\nTo keep the dataset tractable, the module prunes low-utility cells per tile. A simple threshold (e.g. `s < 0.01`) is\navailable, but D2.3 recommends keeping only the Topâ€‘`X` cells (default `Xâ€¯=â€¯10`) for each tile:\n![Top-X pruning example](/img/eu-compliance/topx-pruning.png)\n### Module 5 â€” Cell connection probability estimation\nFootprint values become probabilistic connection weights assuming no load balancing:\n![Connection probability formula](/img/eu-compliance/connection-probability.png)\nwhere `A` is the set of all cells that reach tile `g`. For every tile the probabilities sum to one and serve as priors for\nlater Bayesian localisation (Moduleâ€¯6). The outputs are stored as `Cell Connection Probabilities [INTERMEDIATE RESULTS]`.\n> **Reference:** Tennekes, M. & Gootzen, Y. (2022). *Bayesian location estimation of mobile devices using a signal strength model.* Journal of Spatial Information Science.\n>\n## 2. Event-level quality assurance\n- **Module 7 â€“ Event data syntactic cleaning**  \n  Deduplicates records, orders events temporally, validates mandatory fields, and harmonises roaming identifiers.\n- **Module 8 â€“ Event syntactic quality warnings**  \n  Computes error rates and completeness metrics; emits daily dashboards with configurable thresholds.\n- **Module 9 â€“ Device demultiplex**  \n  Splits cleaned events into per-device streams, aligning by user ID and timestamp while preserving metadata needed downstream.\n- **Module 10 â€“ Device-level semantic cleaning**  \n  Removes impossible jumps (speed constraints, missing cells), imputes short gaps, and tags residual anomalies.\n- **Module 11 â€“ Device activity statistics**  \n  Derives per-device indicators (event counts, idle gaps, night/day ratios) used for filtering and weighting.\n- **Module 12 â€“ Device semantic quality warnings**  \n  Aggregates Moduleâ€¯10 & 11 scores into warning flags that can pause a pipeline or trigger manual review.\n## 3. Daily & longitudinal analytics\n- **Module 13 â€“ Daily processing**  \n  Offers three interchangeable methods: present population estimation, daily permanence score, and continuous time segmentation (CTS). Each consumes semantically cleaned events and connection probabilities to produce daily summaries and state labels.\n- **Module 14 â€“ Mid-term processing**  \n  Aggregates daily permanence scores into monthly or quarterly summaries; implements decay rules, holiday adjustments, and confidence scores.\n- **Module 15 â€“ Long-term processing**  \n  Builds yearly permanence and usual-environment labels, including home/work classification, stay frequencies, and uncertainty indicators.\n- **Module 16 â€“ Tourism methods**  \n  Adapts daily outputs for inbound/outbound tourism: trip detection, nights spent, multi-destination chains, and country-of-origin logic. Validates robustness against multi-roaming scenarios.\n## 4. Aggregation, fusion & estimation\n- **Module 17 â€“ Device filtering & single-MNO aggregation**  \n  Applies quality filters (minimum activity, residence confirmation) and aggregates devices to grid/time buckets with disclosure-aware measures.\n- **Module 18 â€“ Merge single-MNO aggregates**  \n  Combines results from participating MNOs using either secure enclave fusion or exchange of already-protected aggregates; harmonises schemas and metadata.\n- **Module 19 â€“ Projection to use-case geography**  \n  Projects grid counts to administrative or analytical zones (LAU, municipality, FUA) using the same spatial transformation rules referenced in Vol.â€¯I.\n- **Module 19 (Estimation stage) â€“ Calibration of outputs**  \n  Scales MultiMNO totals to official population frames via reweighting, residual adjustments, and estimation of coverage gaps (includes default inference recipes per use case).\n- **Home-location change detection (Chapterâ€¯22)**  \n  Detects migrations by comparing sequential long-term labels with stability thresholds, enabling internal migration statistics.\n## 5. Data objects catalogue\nVolumeâ€¯III concludes with a detailed schema inventory (Chapterâ€¯23) covering:\n- **Input data** â€“ raw network topology, event logs, auxiliary datasets (holidays, MCC/MNC mappings, land use priors).\n- **Intermediate results** â€“ every bronze/silver artefact referenced in the pipeline (cleaned events, quality metrics, grid probabilities, daily/mid-term/long-term summaries).\n- **Output datasets** â€“ gold tables per use case, each with mandatory metadata (time period, spatial reference, quality flags, disclosure level).\n## Telcofy implementation notes\n1. **Module parity** â€” keep the same numbering in code repositories and documentation so auditors can cross-check against D2.3 Vol.â€¯III.  \n2. **Configurability** â€” surface key parameters (speed thresholds, permanence windows, calibration weights) in configuration files, mirroring Eurostat defaults but explicitly tracking Telcofy overrides.  \n3. **Quality evidence** â€” persist syntactic and semantic warning outputs; these artefacts demonstrate compliance with the ESS quality framework and feed customer-facing dashboards.  \n4. **Privacy controls** â€” respect the separation between single-MNO processing, secure enclaves, and post-aggregation SDC as defined in Modulesâ€¯17â€“20. Telcofy add-ons must not bypass these boundaries.\nSee  [D2.3 Vol.â€¯III](https://cros.ec.europa.eu/group/6/files/2656/download) for algorithmic detail, pseudocode, and full attribute definitions."
    },
    {
      "path": "eu-compliance/quality.md",
      "content": "# Quality Assurance for MultiMNO Deliveries\nDeliverable **[D3.3](https://cros.ec.europa.eu/group/6/files/2659/download)** sets out the final business process model and quality framework for MultiMNO. It complements the methodological, use-case, and methods documentation (D2.3 Vols.â€¯Iâ€“III) with concrete expectations on how MNO-based statistics are governed, monitored, and reported. Below is the subset that guides Telcofyâ€™s quality controls; consult the [Eurostat deliverables overview](https://cros.ec.europa.eu/book-page/methodology-framework-high-level-architecture-requirements-use-cases-and-methods) for the full text.\n## ESS quality context\n- **Conceptual baseline** â€” D3.3 reiterates the European Statistics Code of Practice (ES CoP) and the ESS Quality Assurance Framework (QAF) as the reference standards. It inventories which ES CoP principles and QAF indicators already apply to MNO data and where tailored interpretations or extensions are needed.\n- **Quality toolbox** â€” The report reviews existing ESS tools (SIMS metadata, quality reports, quality indicators dashboards) and explains how they should be adapted for telecom-derived statistics.\n## Input data quality (MNO-supplied feeds)\n- **Network topology** and **event data** receive separate checklists with concrete metrics (missing values, format errors, out-of-range values, parsing problems).\n- D3.3 proposes a *quality report template* for MNOs, capturing:\n  - Dataset identifiers, temporal coverage, and sampling notes.\n  - Overview metrics (row counts, error ratios) and thresholds that trigger warnings.\n  - Ticketing workflow references when data defects are found.\n- A collaboration agreement template (appendix) specifies the division of responsibilities between MNOs and NSIs, including SLA-style commitments for delivering corrected data.\n## Throughput quality (pipeline processing)\nThe quality framework maps each functional module in the reference pipeline (Modulesâ€¯1â€“19) to specific controls:\n- **Pre-processing** â€” syntactic/semantic QC for topology and event data (aligned with D2.3 Vol.â€¯III Modulesâ€¯1â€“12).\n- **Analytical modules** â€” recommended validation metrics for Daily Processing (CTS, DPS, Present Population), Mid-term/Long-term permanence, Tourism modules, etc.\n- **Automation** â€” ticketing system guidelines ensure that quality issues raised by modules feed directly into incident tracking and resolution.\n## Output validation & reporting\n- **Indicators** â€” The report proposes key quality indicators (KQIs) for present population, usual environment, tourism, and internal migration outputs. Examples include coverage ratios, stability indices, and comparisons against official benchmarks.\n- **Total Error Framework** â€” D3.3 adopts a Total Error perspective (updating the traditional survey TSE) to highlight where measurement, processing, or integration errors may enter the MNO pipeline.\n- **Quality reports** â€” An ESS-compliant template is provided, extending SIMS metadata to cover:\n  - Method descriptions, parameter choices, and data sources.\n  - Quality dimensions (relevance, accuracy, timeliness, coherence, accessibility).\n  - Risk assessments and mitigation steps.\n## Business process model\n- D3.3 details a MultiMNO-specific business process mapped to the Generic Statistical Business Process Model (GSBPM). Key elements include:\n  - Preparation and legal arrangements.\n  - Data acquisition, processing, and analysis workflows across MNOs and NSIs.\n  - Publication and archiving stages, including data retention requirements.\n- The model highlights decision points, responsibilities, and the recommended *ticketing system* to coordinate MNO and NSI teams.\n## Software quality considerations\n- The report addresses software governance: version control, code review, testing regimes, and deployment practices for the reference implementation.\n- Telcofy should mirror these controls (e.g., CI pipelines, automated tests, package audits) to demonstrate compliance during audits.\n## Telcofy adoption checklist\n1. **Embed QAF indicators** â€” Implement the suggested metrics for topology, events, and module outputs; surface them in dashboards accessible to QA reviewers.\n2. **Adopt MNO quality reports** â€” Ensure data-sharing agreements include the D3.3 template and enforce delivery of quality annexes with each data exchange.\n3. **Integrate ticketing** â€” Route automated quality warnings (from Modulesâ€¯1â€“19) into Telcofyâ€™s incident/ticketing platform with traceable resolution workflows.\n4. **Publish SIMS-aligned metadata** â€” When releasing statistics to EU stakeholders, package outputs with the extended SIMS quality report defined in D3.3.\n5. **Align business processes** â€” Map Telcofyâ€™s internal RACI matrix to the GSBPM-based process (data acquisition, processing, publication) described in the deliverable.\n6. **Maintain software QA** â€” Record tests, code reviews, and release notes to mirror the software quality expectations in D3.3.\nBy following these guidelines Telcofy can show that its end-to-end production system not only uses the MultiMNO methodology, but also meets the ESS quality obligations laid out in D3.3."
    },
    {
      "path": "eu-compliance/use-cases.md",
      "content": "# MultiMNO Use Case Portfolio\nDeliverable **[D2.3 Volume II](https://cros.ec.europa.eu/group/6/files/2656/download)** inventories the statistical products that Eurostatâ€™s MultiMNO pipeline can generate from mobile network operator (MNO) data. Telcofy reuses these definitions when mapping our offerings to EU stakeholder expectations. The sections below condense the seven headline use cases (with sub-variants) and highlight their operational implications. (See the [CROS deliverables overview](https://cros.ec.europa.eu/book-page/methodology-framework-high-level-architecture-requirements-use-cases-and-methods) for the full documentation set.)\n## UC 1.A â€” Present Population Estimation\n- **Output**: headcount of all persons physically present in a spatial unit at a given timestamp; optionally extended with visit frequency or dwell time.  \n- **Spatial scope**: INSPIRE grid (100Ã—100â€¯m) as processing baseline; output can be reprojected to any NSI-defined zoning (census areas, municipalities, etc.).  \n- **Temporal resolution**: configurable snapshots (e.g., every two hours); reference scenario targets daily coverage, while demonstrator runs may focus on exemplar days/times.  \n- **Value**: supports census validation, emergency planning, social policy, and environmental accounts by exposing near-real-time population distribution, including non-residents.\n## UC 2 â€” Usual Environment & Derived Indicators\n### UC 2.A â€” M-Usual Environment (M-UE)\n- Identifies the set of locations a person frequents (home, work/study, secondary homes, â€œunspecified usual placesâ€).  \n- Aggregates by grid or administrative geography to quantify how many people anchor their routine in each area, how many usual places they maintain, and the distances between them.  \n- Requires longitudinal observation (recommended 12 months) to capture seasonality and to guard against misclassification.\n### UC 2.B â€” M-Home Location\n- Derives residential population estimates exclusively from behavioural data (time spent, overnight presence) instead of subscription addresses.  \n- Outputs can be stratified by age group, socio-economic proxies, or housing type once integrated with auxiliary registers.  \n- Useful for census frame updates, housing policy, and monitoring second-home usage.\n### UC 2.C â€” Access to Services\n- Combines M-UE footprints with service locations (schools, hospitals, retail) to profile accessibility and service catchments.  \n- Produces indicators such as travel time distributions, share of population within defined service radii, and identification of underserved areas.  \n- Informs spatial justice debates, infrastructure planning, and resilience assessments.\n## UC 3 â€” Mobility Profiles\n### UC 3.A â€” Usual Mobility (M-UM)\n- Tracks the recurring flows between usual environment locales (e.g., typical daily itineraries, clusters of grid tiles).  \n- Outputs include dominant movement chains, distance bands, and temporal rhythms (weekday vs. weekend, seasonal shifts).  \n- Downstream use: transport modelling, functional area delineation, and validation of smart city interventions.\n### UC 3.B â€” Commuting\n- Focuses on home-to-work/study journeys inferred from M-UE labelling.  \n- Delivers originâ€“destination matrices, modal proxies (derived from travel speed and corridor selection), and peak-load diagnostics.  \n- Supports labour market intelligence, public transit scheduling, and greenhouse-gas accounting.\n## UC 4 â€” M-Functional Urban Areas (M-FUA) & Greater City Boundaries\n- Fuses M-UE and M-UM indicators to delineate urban cores and commuting zones based on observed mobility rather than static administrative limits.  \n- Produces alternative FUA geometries, dynamic â€œgreater cityâ€ extents, and population tallies per zone.  \n- Enables NSIs to refresh national FUA lists, benchmark against Eurostat definitions, and study peri-urban expansion.\n## UC 5 â€” Tourism Statistics\n### UC 5.A â€” Domestic Touristic Arrivals & Nights Spent\n- Detects overnight trips outside the individualâ€™s usual environment but within the country.  \n- Reports arrivals, nights, and multi-destination itineraries at fine spatial resolution; can differentiate leisure vs. business using dwell-time heuristics plus auxiliary data.\n### UC 5.B â€” Domestic Same-Day Visits\n- Captures excursions without overnight stays by flagging out-of-UE travel and same-day return.  \n- Outputs include visitor counts, average stay duration, and trip purpose proxies.\n### UC 5.C â€” Inbound Tourism\n- Monitors foreign devices roaming on domestic networks; classifies country of origin via MCC/MNC and identifies inbound travel patterns.  \n- Requires careful handling of multi-roaming behaviours (devices switching between operators) and collaboration with MNOs for complete coverage.  \n- Outputs feed balance-of-payments, regional tourism boards, and crisis response (e.g., pandemic border controls).\n### UC 5.D â€” Outbound Tourism\n- Follows domestic subscribers abroad by leveraging outbound roaming records.  \n- Produces counts of travellers, nights spent, visited countries/regions, and trip chains.  \n- Complements UC 5.C to deliver a full inbound/outbound tourism satellite account.\n## UC 6 â€” Exposure to Risks\n- Integrates present population, usual environment, and hazard layers (natural disasters, pollution, industrial sites) to quantify population exposure.  \n- Supports dynamic risk maps, early warning prioritisation, and impact assessment of mitigation measures.  \n- Requires prompt data provisioning to civil protection agencies while maintaining statistical disclosure controls.\n## UC 7 â€” Internal Migration\n- Uses long-term permanence scores and successive home-location labels to detect relocations between administrative units.  \n- Generates migration flows, net balances, and duration-of-stay statistics; can distinguish temporary vs. permanent moves by observing post-move stability.  \n- Benefits demographic analysis, housing supply forecasts, and regional funding formulas.\n---\n**Telcofy action points**\n1. **Metadata parity** â€” ensure our product catalogue references the same UC IDs and terminology when interacting with NSIs or Eurostat.  \n2. **Parameter transparency** â€” document UE, UM, and tourism thresholds so auditors can reconcile Telcofy outputs with MultiMNO defaults.  \n3. **Quality + uncertainty** â€” propagate the soft-classification paradigm (confidence scores, warning flags) to every UC export.  \n4. **Privacy posture** â€” preserve the disclosure-control assumptions (basic vs. advanced data fusion) when configuring data sharing agreements."
    },
    {
      "path": "api/authentication.md",
      "content": "# Telcofy Authentication Guide\nExternal customers authenticate to Telcofy services with **machine-account API keys**\nissued by the Telcofy team. Two base URLs are available:\n- Users API: `https://users.api.telcofy.ai`\n- Data API: `https://data.api.telcofy.ai`\nThe Users API exchanges API keys for short-lived Google OAuth tokens (for Cloud Storage\ndownloads). The Data API accepts the same API key via the `x-api-key` header.\n## 2. Exchange the API Key for an OAuth Token\n```bash\nAPI_KEY=\"YOUR_API_KEY\"\nLOGIN_RESPONSE=$(curl -s -X POST https://users.api.telcofy.ai/login-with-apikey \\\n  -H \"x-api-key: $API_KEY\")\nACCESS_TOKEN=$(echo \"$LOGIN_RESPONSE\" | jq -r .accessToken)\nUSER_ID=$(echo \"$LOGIN_RESPONSE\" | jq -r .userId)\nEXPIRES_AT=$(echo \"$LOGIN_RESPONSE\" | jq -r .expireTime)\n```\n- Tokens are valid for roughly one hour; check `expireTime` in the response.\n- The default scope is `https://www.googleapis.com/auth/devstorage.read_only`.\n- Provide a JSON body to request a different scope that matches your machine-account\n  profile (for example, `{\"service\": \"bigquery\"}` to mint `https://www.googleapis.com/auth/bigquery`).\n```bash\n# Request a BigQuery-scoped token\nLOGIN_RESPONSE=$(curl -s -X POST https://users.api.telcofy.ai/login-with-apikey \\\n  -H \"x-api-key: $API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"service\":\"bigquery\"}')\n```\nUse the bearer token with Cloud Storage tooling. The examples below rely on the `USER_ID`\nreturned by `/login-with-apikey`.\n**Example (gsutil):**\n```bash\ngsutil -o \"GSUtil:additional_http_headers=Authorization: Bearer $ACCESS_TOKEN\" \\\n  cp -r gs://telcofy-user-data/results/$USER_ID/ ./results_$USER_ID\n```\n**Example (HTTP download):**\n```bash\ncurl -L -H \"Authorization: Bearer $ACCESS_TOKEN\" \\\n  \"https://storage.googleapis.com/telcofy-user-data/results/$USER_ID/export.json\" \\\n  -o export.json\n```\n---\n## 3. Calling the Admin API with an API Key\nAttach your API key to each Telcofy admin request using the `x-api-key` header.\n### Admin Maps (`/admin/maps`)\nAdmin map endpoints let you store and maintain reusable geometries. These routes live on\n`https://users.api.telcofy.ai` and also use the `x-api-key` header.\n| Endpoint | Description | Notes |\n| --- | --- | --- |\n| `GET /admin/maps` | List saved admin maps (custom polygons, grids). | Returns map metadata, including geometry and owning machine account. |\n| `GET /admin/maps/monitored` | List admin map IDs that are flagged for **realtime monitoring**. | Returns `count` plus `monitored_map_ids`. |\n| `POST /admin/maps` | Create a new admin map. | Provide `name`, `type`, and either `geometry` (WKT) or compatible `ids`. |\n| `PUT /admin/maps/:id` | Update an existing admin map. | Supply only the fields you want to change; geometry updates replace the stored polygon. |\n| `DELETE /admin/maps/:id` | Delete an admin map. | Removes the map from future queries; returns `{ \"msg\": \"Map deleted\" }`. |\n**List saved maps:**\n```bash\ncurl -s https://users.api.telcofy.ai/admin/maps \\\n  -H \"x-api-key: $API_KEY\"\n```\nExample response:\n```json\n  \"maps\": [\n      \"id\": \"2nnuJGDA0axOeuafA0wy\",\n      \"name\": \"Customer Zone Alpha\",\n      \"description\": \"Example custom zone created via API key\",\n      \"type\": \"custom_polygon\",\n      \"geometry\": \"POLYGON((10.7330245 59.948585, 10.734826 59.948413, 10.736222 59.949133, 10.735642 59.949885, 10.733625 59.94995, 10.732315 59.94924, 10.7330245 59.948585))\",\n      \"owner\": \"api-test-user-1-my-dev-key@telcofy-norway-poc.iam.gserviceaccount.com\"\n  ]\n```\n**List monitored maps:**\n```bash\ncurl -s https://users.api.telcofy.ai/admin/maps/monitored \\\n  -H \"x-api-key: $API_KEY\"\n```\nExample response:\n```json\n{ \"count\": 1, \"monitored_map_ids\": [\"2nnuJGDA0axOeuafA0wy\"] }\n```\n**Create a map:**\n```bash\ncurl -s -X POST https://users.api.telcofy.ai/admin/maps \\\n  -H \"x-api-key: $API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"name\": \"Customer Zone Alpha\",\n        \"description\": \"Example custom zone created via API key\",\n        \"type\": \"custom_polygon\",\n        \"geometry\": \"POLYGON((10.7872664 59.8679278, 10.7969259 59.8680208, 10.7969259 59.8701131, 10.7924594 59.8734470, 10.7878905 59.8708231, 10.7872664 59.8679278))\"\n      }'\n```\nExample response:\n```json\n  \"msg\": \"Map saved\",\n  \"id\": \"2nnuJGDA0axOeuafA0wy\",\n  \"name\": \"Customer Zone Alpha\",\n  \"description\": \"Example custom zone created via API key\",\n  \"type\": \"custom_polygon\",\n  \"geometry\": \"POLYGON((10.7872664 59.8679278, 10.7969259 59.8680208, 10.7969259 59.8701131, 10.7924594 59.8734470, 10.7878905 59.8708231, 10.7872664 59.8679278))\",\n  \"owner\": \"api-test-user-1-my-dev-key@telcofy-norway-poc.iam.gserviceaccount.com\"\n```\n**Update a map:**\n```bash\ncurl -s -X PUT https://users.api.telcofy.ai/admin/maps/$MAP_ID \\\n  -H \"x-api-key: $API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"name\": \"Customer Zone Alpha (updated)\",\n        \"description\": \"Polygon geometry updated via API key\",\n        \"type\": \"custom_polygon\",\n        \"geometry\": \"POLYGON((10.761452 59.914762, 10.7654 59.914762, 10.7654 59.916699, 10.760765 59.916699, 10.757761 59.916139, 10.761452 59.914762))\"\n      }'\n```\nSuccessful updates return the map ID:\n```json\n{ \"msg\": \"Map updated\", \"id\": \"svxtowUIKG33pVmbXKBT\" }\n```\n**Delete a map:**\n```bash\ncurl -s -X DELETE https://users.api.telcofy.ai/admin/maps/$MAP_ID \\\n  -H \"x-api-key: $API_KEY\"\n```\nExample response:\n```json\n{ \"msg\": \"Map deleted\", \"id\": \"QdZn1VqNBreF1Lyoc9Hj\" }\n```\n### Realtime Admin API \nEnable or disable realtime monitoring for previously saved admin maps. These calls use\n`https://data.api.telcofy.ai` and require the same `x-api-key` header.\n| Endpoint | Description | Notes |\n| --- | --- | --- |\n| `POST /realtime` | Toggle realtime monitoring for a map. | Body accepts `map_id` and `enable` (`true`/`false`); updates the realtime monitoring service parameters. |\n**Enable monitoring for a map:**\n```bash\ncurl -s -X POST https://data.api.telcofy.ai/realtime \\\n  -H \"x-api-key: $API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{ \"map_id\": \"svxtowUIKG33pVmbXKBT\", \"enable\": true }'\n```\nExample response:\n```json\n  \"msg\": \"Realtime monitoring enabled\",\n  \"map_id\": \"svxtowUIKG33pVmbXKBT\",\n  \"is_monitored\": true,\n  \"bq_updated_rows\": 1\n```\n### Data Aggregation API (`/data-agg`)\n//UNDER DEVELOPMENT\nUse `/data-agg` to request Telcofyâ€™s analytical products - Activities and Origin-Destination Matrix (ODM) datasets.\nSubmit asynchronous jobs for large date ranges, then download the results once processing completes. See\n[`data-access/overview`](../data-access/overview.md) for a deeper look at product definitions and available measures.\n**Submit an async aggregation job:**\n```bash\ncurl -s -X POST https://data.api.telcofy.ai/data-agg \\\n  -H \"x-api-key: $API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{ \"agg_type\": \"activities\", \"measure\": \"sum_unique_people\", \"start_time\": \"2024-03-01T00:00:00Z\", \"end_time\": \"2024-03-01T23:59:59Z\", \"activity_type\": \"daily\", \"geo_type\": \"grid_1000m\", \"geo_ids\": [22637506648500], \"full\": true }'\n```\nPoll `/data-agg/status/{jobId}` and `/data-agg/results/{jobId}` using the same header.\n---\n## 4. Security Best Practices\n- Treat API keys as secretsâ€”do not embed them in mobile apps, client-side code, or public repositories.\n- Rotate keys periodically (quarterly recommended) and immediately when staff change roles.\n- Restrict Cloud Storage downloads to secured environments; OAuth tokens inherit read-only access to your results folder.\n- If an API key is suspected to be compromised, request an immediate rotation from Telcofy support.\n- Audit usage via `/users/:uid/machine-accounts/list` or Telcofy-provided dashboards when available.\nStay secure!"
    },
    {
      "path": "api/endpoints.md",
      "content": "# Telcofy REST Endpoints\nQuick reference to Telcofyâ€™s externally exposed REST endpoints.\nAll paths are relative to:\n- Users API â€“ `https://users.api.telcofy.ai`\n- Data API â€“ `https://data.api.telcofy.ai`\nSee `authentication.md` for header requirements and token exchange details.\n## Data API\n| Method | Path | Description | Auth |\n| ------ | ---- | ----------- | ---- |\n| GET | `/data-agg` | Run a synchronous aggregation (`activities` or `trips`). | `x-api-key` |\n| POST | `/data-agg` | Submit an aggregation job (`full=true` exports to Cloud Storage). | `x-api-key` |\n| GET | `/data-agg/status/:jobId` | Check aggregation job status and progress. | `x-api-key` |\n| GET | `/data-agg/results/:jobId` | Retrieve preview results or export metadata (`?full=true`). | `x-api-key` |\n| GET | `/data-agg/jobs` | List historical aggregation jobs for the caller. | `x-api-key` |\n| POST | `/realtime` | Enable or disable realtime monitoring for a saved map. | `x-api-key` |\n---\n## Response Formats\n- All responses are JSON encoded and include an `error` field on failures.\n- Timestamp fields follow ISO 8601 (`YYYY-MM-DDTHH:mm:ss.sssZ`) unless noted.\n- Long-running jobs surface progress via `/data-agg/status/:jobId` and deliver\nRefer to `quickstart.md` for example requests and to the individual service\nreadmes for in-depth payload schemas."
    },
    {
      "path": "api/quickstart.md",
      "content": "# Telcofy Data Access Quickstart\nUse this quickstart to exchange your Telcofy machine-account API key for an OAuth\ntoken, download Cloud Storage exports, and call the Data API for maps and\naggregated metrics. The Users API issues Google OAuth tokens scoped to specific\nservices: Cloud Storage by default, or BigQuery when explicitly requested.\nTo obtain Telcofy API access you need a valid contract; contact sales at tom@telcofy.ai.\n## 2. Exchange the API Key for a Cloud Storage Access Token\nIf you are unsure where to retrieve `YOUR_MACHINE_ACCOUNT_KEY`, see the **API key\nprovisioning** notes in [`authentication.md`](authentication.md).\nCall `POST /login-with-apikey` to obtain a one-hour Google OAuth token. The\nresponse includes both the token and the Telcofy `userId` that owns the data.\nInclude an optional JSON body with `{\"service\":\"bigquery\"}` when you need a\nBigQuery-scoped token instead of the default Cloud Storage scope.\n```bash\nLOGIN_RESPONSE=$(curl -s -X POST https://users.api.telcofy.ai/login-with-apikey \\\n  -H \"x-api-key: $API_KEY\")\nACCESS_TOKEN=$(echo \"$LOGIN_RESPONSE\" | jq -r .accessToken)\nUSER_ID=$(echo \"$LOGIN_RESPONSE\" | jq -r .userId)\n# Request a BigQuery-scoped token instead of Cloud Storage:\n# LOGIN_RESPONSE=$(curl -s -X POST https://users.api.telcofy.ai/login-with-apikey \\\n#   -H \"x-api-key: $API_KEY\" \\\n#   -H \"Content-Type: application/json\" \\\n#   -d '{\"service\":\"bigquery\"}')\n# ACCESS_TOKEN=$(echo \"$LOGIN_RESPONSE\" | jq -r .accessToken)\n```\nTokens expire after ~1 hour; repeat the exchange whenever you encounter\nauthorization errors.\n---\n## 3. Download Cloud Storage Exports\nUse the OAuth token as a bearer credential. The example below copies every file\nunder your userâ€™s export directory using the `USER_ID` captured in the previous\nstep.\n```bash\ngsutil -o \"GSUtil:additional_http_headers=Authorization: Bearer $ACCESS_TOKEN\" \\\n  cp -r gs://telcofy-user-data/results/$USER_ID/ ./results_$USER_ID\n```\nPrefer a pure HTTP workflow? Use `curl` to stream individual objects:\n```bash\ncurl -L -H \"Authorization: Bearer $ACCESS_TOKEN\" \\\n  \"https://storage.googleapis.com/telcofy-user-data/results/$USER_ID/example.json\" \\\n  -o example.json\n```\nDownload an entire folder with `curl` by iterating over the object list:\n```bash\nFILES=$(gsutil -o \"GSUtil:additional_http_headers=Authorization: Bearer $ACCESS_TOKEN\" \\\n  ls gs://telcofy-user-data/results/$USER_ID/**)\nmkdir -p results_$USER_ID\nfor f in $FILES; do\n  NAME=$(basename \"$f\")\n  curl -L -H \"Authorization: Bearer $ACCESS_TOKEN\" \"$f\" -o \"results_$USER_ID/$NAME\"\ndone\n```\nPrefer Python? Use the Google Cloud Storage client with the temporary OAuth token:\n```python\nfrom google.cloud import storage\nfrom google.oauth2.credentials import Credentials\nAPI_KEY = os.environ[\"API_KEY\"]\nBASE_URL = \"https://users.api.telcofy.ai\"\nBUCKET_NAME = \"telcofy-user-data\"\nresp = requests.post(f\"{BASE_URL}/login-with-apikey\", headers={\"x-api-key\": API_KEY})\nresp.raise_for_status()\ntoken_data = resp.json()\ncreds = Credentials(token=token_data[\"accessToken\"])\nclient = storage.Client(credentials=creds)\nuser_id = token_data[\"userId\"]\nprefix = f\"results/{user_id}/\"\nos.makedirs(f\"results_{user_id}\", exist_ok=True)\nfor blob in client.list_blobs(BUCKET_NAME, prefix=prefix):\n    if blob.name.endswith(\"/\"):\n        continue\n    local = os.path.join(f\"results_{user_id}\", os.path.basename(blob.name))\n    blob.download_to_filename(local)\n    print(f\"Downloaded {blob.name} -> {local}\")\n```\n## 4. Download data from Bigquery shared datasets\nPrefer Python for querying shared BigQuery datasets? Request a BigQuery-scoped token\nand run parameterised queries against the Shared BigQuery dataset (see\n[`Bigquery Sharing`](../data-access/analytical-hub.md) for dataset names and schema details):\n```python\nfrom google.cloud import bigquery\nfrom google.oauth2.credentials import Credentials\nAPI_KEY = os.environ[\"API_KEY\"]\nBASE_URL = \"https://users.api.telcofy.ai\"\n# Step 1: Get a BigQuery-scoped token\nresp = requests.post(\n    f\"{BASE_URL}/login-with-apikey\",\n    headers={\"x-api-key\": API_KEY},\n    json={\"service\": \"bigquery\"},\n)\nresp.raise_for_status()\ntoken_data = resp.json()\ncreds = Credentials(token=token_data[\"accessToken\"])\n# Step 2: Initialise the BigQuery client with the temporary credentials\nclient = bigquery.Client(project=\"YOUR_SHARED_PROJECT\", credentials=creds)\n# Step 3: Query Telcofy analytical hub views\nquery = \"\"\"\n    SELECT\n      c.target_name, \n      c.timestamp, \n      c.people_count  \n    FROM `YOUR_SHARED_PROJECT.YOUR_DATASET.realtime_summary` c\n    WHERE c.timestamp >= '2025-10-10T00:00:00Z' \n    LIMIT 10\n\"\"\"\nfor row in client.query(query):\n    print(\n        f\"{row.target_name} | {row.timestamp} | {row.people_count} \"\n    )\n```\nNeed the same scope from the CLI? Include the JSON payload when calling the login endpoint:\n```bash\ncurl -s -X POST https://users.api.telcofy.ai/login-with-apikey \\\n  -H \"x-api-key: $API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"service\":\"bigquery\"}'\n```\n---\n## 5. Call the Data API (Realtime & Aggregations)\nThe Data API accepts your Telcofy API key via the `x-api-key` header.\nToggle realtime monitoring for a saved admin map:\n```bash\ncurl -s -X POST https://data.api.telcofy.ai/realtime \\\n  -H \"x-api-key: $API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{ \"map_id\": \"2nnuJGDA0axOeuafA0wy\", \"enable\": false }'\n```\nExample response:\n```json\n  \"msg\": \"Realtime monitoring disabled\",\n  \"map_id\": \"2nnuJGDA0axOeuafA0wy\",\n  \"is_monitored\": false,\n  \"bq_updated_rows\": 1\n```\nFetch a synchronous population aggregation:\n// THIS FEATURE UNDER DEVELOPMENT AND NOT AVAILABLE IN PRODUCTION\n```bash\ncurl -s \"https://data.api.telcofy.ai/data-agg?agg_type=activities&measure=sum_unique_people&start_time=2024-03-01T08:00:00Z&end_time=2024-03-01T09:00:00Z&activity_type=hourly&geo_type=grid_250m&geo_ids=22637506648500\" \\\n  -H \"x-api-key: $API_KEY\" | jq '.results[0]'\n```\nSubmit a long-running aggregation job (exports to Cloud Storage when `full=true`):\n```bash\ncurl -s -X POST https://data.api.telcofy.ai/data-agg \\\n  -H \"x-api-key: $API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"agg_type\": \"activities\",\n        \"measure\": \"sum_unique_people\",\n        \"start_time\": \"2024-03-01T00:00:00Z\",\n        \"end_time\": \"2024-03-01T23:59:59Z\",\n        \"activity_type\": \"daily\",\n        \"geo_type\": \"grid_1000m\",\n        \"geo_ids\": [22637506648500, 22640006648500],\n        \"full\": true\n      }'\n```\nPoll `/data-agg/status/{jobId}` and `/data-agg/results/{jobId}` until the job\ncompletes, then download the exported files from Cloud Storage using the OAuth\ntoken retrieved earlier.\n---\n## 6. Keep Exploring\n- Review `authentication.md` for API key exchange details and best practices.\n- Consult `endpoints.md` for the full list of supported Users API and Data API routes.\n- Reach out to your Telcofy account team if you need additional dataset access or\n  assistance with automation.\nHappy building!"
    },
    {
      "path": "api/webhooks.md",
      "content": "# Webhooks\nComing soon..."
    },
    {
      "path": "data-access/analytical-hub.md",
      "content": "# BigQuery Sharing\nTelcofy distributes curated telecom intelligence as reusable BigQuery data exchanges. By leveraging [BigQuery Sharing](https://cloud.google.com/bigquery/docs/analytics-hub-introduction), you can subscribe to governed datasets directly inside Google Cloud, eliminating the overhead of manual data copies and keeping all consumers aligned on a single, auto-updating source of truth.\n## Why use BigQuery Sharing listings\n- **Native BigQuery experience** â€” query Telcofy tables with standard SQL, join them with your internal assets, and manage access through IAM.\n- **Consistent updates** â€” subscribed listings always point to the latest Telcofy-managed data, so you avoid drift across teams and environments.\n- **Governance and observability** â€” Google Cloud's audit logs, tagging, and lineage features flow through to the shared datasets, simplifying compliance reviews.\n## Prerequisites\n- A Google Cloud project with BigQuery enabled in the `eu` multi-region.\n- Permission to view and subscribe to Telcofy's `realtime_customer_views` listing.\n- (For API access) The `analyticsHub.googleapis.com` service enabled plus credentials with the `bigquery.dataViewer` and `analyticshub.listings.subscribe` permissions.\n## Explore available listings\nSign in to the BigQuery console and open **BigQuery Sharing** from the left navigation. Search for *Telcofy* to locate the `realtime_customer_views` exchange or browse via the direct link shared by the Telcofy team. The [Google Cloud guide on viewing and subscribing to listings](https://cloud.google.com/bigquery/docs/analytics-hub-view-subscribe-listings#subscribe-listings) walks through the UI elements if you are new to the workspace.\n## Subscribe to Telcofy's listing\nBigQuery Sharing supports both console-based subscriptions and fully automated flows through the REST API.\n### Option A: BigQuery console\n1. In the BigQuery console, open **BigQuery Sharing** and navigate to the `Telcofy / Real-time Customer Views` listing.\n2. Review the dataset details, including the data refresh cadence and usage terms.\n3. Click **Subscribe**. Choose an existing project and dataset location (must be `EU`) where the linked dataset should live.\n4. Confirm. BigQuery creates a **linked dataset** in your project that mirrors the Telcofy tables. You can now run SQL queries in the `bigquery-public-data` style using the dataset name you selected.\n### Option B: REST API\nAutomate onboarding by calling the BigQuery Sharing API endpoint:\n```bash\ncurl -X POST \\\n  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n  -H \"Content-Type: application/json\" \\\n  \"https://analyticshub.googleapis.com/v1/projects/telcofy-norway-poc/locations/eu/dataExchanges/realtime_customer_views/listings/realtime_customer_views_19a0c4cec2a:subscribe\" \\\n  -d '{\n    \"destinationDataset\": {\n      \"datasetReference\": {\n        \"projectId\": \"YOUR_PROJECT_ID\",\n        \"datasetId\": \"LINKED_DATASET_NAME\"\n      },\n      \"location\": \"EU\"\n  }'\n```\nThis request subscribes the specified project to the `realtime_customer_views` listing and creates (or reuses) the linked dataset. Ensure that the caller has the required IAM roles and that the destination dataset location matches the listing region.\nOnce subscribedâ€”via either methodâ€”query the linked dataset from BigQuery, schedule downstream jobs, or grant access to additional teammates through standard IAM policies."
    },
    {
      "path": "data-access/overview.md",
      "content": "# Data Delivery Methods Overview\nTelcofy offers several pathways to explore, export, and integrate telecom insights. Choose the delivery mechanism that best fits your workflow, whether you prefer file-based exploration, SQL-driven analysis, or near-real-time API calls.\nAll Telcofy data products can be accessed and configured through the [Telcofy CityOS](https://app.telcofy.ai). If you do not yet have console access, reach out to [tom@telcofy.ai](mailto:tom@telcofy.ai) to request a trial.\nTelcofy curates two product families:\n- **Analytical products (Activities, ODMs, â€¦)** â€” export curated files to your cloud storage bucket (see *Storage-backed datasets*), subscribe to governed tables via BigQuery Sharing, or automate retrieval through the REST API with tools like `curl`, Python, or Node.js clients.\n- **Low-latency products (Telcofy Realtime)** â€” refreshed roughly every five minutes and best consumed via BigQuery Sharing or the REST API to keep downstream systems current.\n## Storage-backed datasets\nFor teams that want direct file access, Telcofy curates data drops in familiar formats such as CSV, JSON, and Parquet. These assets are delivered through blob storage so you can pull them into notebooks, lakehouses, or existing ETL pipelines without additional tooling. See [API Reference](../api/quickstart.md) for more details\n## BigQuery Sharing\nIf you already rely on Google Cloud, Telcofy's BigQuery listings provide an interactive SQL experience with governed sharing. Start with the curated listings described in the [BigQuery Sharing guide](./analytical-hub.md) to join, filter, and enrich datasets using native BigQuery tooling.\n## REST API\nFor API-first integrations and operational systems that require programmatic access, Telcofy exposes the same curated data sets through a REST interface. Follow the [REST API quickstart](../api/quickstart.md) to authenticate, browse resources, and embed Telcofy insights into your applications."
    },
    {
      "path": "analytics/custom-queries.md",
      "content": "# Custom Queries\nComing soon..."
    },
    {
      "path": "analytics/dashboard.md",
      "content": "# Dashboard\nComing soon..."
    },
    {
      "path": "analytics/overview.md",
      "content": "# Analytics Overview\nComing soon..."
    },
    {
      "path": "analytics/visualizations.md",
      "content": "# Visualizations\nComing soon..."
    }
  ]
}